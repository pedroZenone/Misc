{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "\n",
    "from melitk.analytics.connectors.core.authentication import Authentication\n",
    "from melitk.analytics.connectors.teradata import ConnTeradata\n",
    "from melitk.analytics.connectors.presto import ConnPresto\n",
    "from melitk.fda import workspace\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from shared.settings import DATASET_FILENAME, SAMPLE\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from lifetimes.utils import summary_data_from_transaction_data\n",
    "import numpy as np\n",
    "import datetime\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "from lifetimes import ModifiedBetaGeoFitter\n",
    "from lifetimes import GammaGammaFitter\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import joblib\n",
    "import boto3\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Process\n",
    "import gensim\n",
    "import multiprocessing as mp\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _write_dataframe_to_csv_on_s3(df, path_s3):\n",
    "    import boto3\n",
    "    from io import StringIO\n",
    "    \"\"\" Write a dataframe to a CSV on S3 \"\"\"\n",
    "    a = path_s3.split('//')\n",
    "    b = a[1].split('/')\n",
    "    bucket = b[0]\n",
    "    c = path_s3.split(bucket+'/')\n",
    "    path = c[1]\n",
    "    \n",
    "    buffer = StringIO()\n",
    "    df.to_csv(buffer,index=False)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Object(bucket, path).put(Body=buffer.getvalue())\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_s3(x,s3_path):\n",
    "    import boto3\n",
    "    import datetime\n",
    "    \n",
    "    print(x)\n",
    "    file = s3_path.split('/')[-1]\n",
    "    with open(file, \"a\") as myfile:\n",
    "        myfile.write(str(datetime.date.today().strftime(\"%m/%d/%Y %H:%M:%S\") + \": \"+x + \"\\n\"))\n",
    "        \n",
    "    s3 = boto3.client('s3')\n",
    "    s3.upload_file(file, \"fda-labs\",  '/'.join(s3_path.split('/')[3:])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.read_csv(\"s3://fda-labs/ltv-ml/ML/summary_visitas_train.csv\",sep = \",\")\n",
    "summary[\"phrase\"] = summary[\"phrase\"].fillna(' ')\n",
    "summary[\"phrase\"] = summary[\"phrase\"].apply(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tagged_document(list_of_list_of_words):\n",
    "    for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "\n",
    "data = [list(x) for x in summary[\"phrase\"].values]\n",
    "train_data = list(create_tagged_document(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_doc2vec(size,window):\n",
    "    model = gensim.models.doc2vec.Doc2Vec(vector_size=size, min_count=0, epochs=40,workers = 8,dm = 1,window = window)\n",
    "    # Build the Volabulary\n",
    "    model.build_vocab(train_data)\n",
    "    # Train the Doc2Vec model\n",
    "    model.train(train_data, total_examples=model.corpus_count, epochs=model.epochs)    \n",
    "    return model\n",
    "\n",
    "\n",
    "params_grid = [(10,8),(10,12),(10,20),(20,8),(20,12),(20,20),(20,8),(32,12),(32,20),(32,8)]\n",
    "shuffle(params_grid)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "for params in params_grid[:4]:\n",
    "    model = model_doc2vec(*params)\n",
    "    name = \"doc2vec_\"+str(params[0])+\"_\"+str(params[1])\n",
    "    model.save(name)\n",
    "    print_s3('||'.join(os.listdir()),\"s3://fda-labs/ltv-ml/Embedding/log.txt\")\n",
    "    \n",
    "    files = [x for x in os.listdir() if(name in x)]\n",
    "    for f in files:\n",
    "        s3.upload_file(f, \"fda-labs\", \"ltv-ml/Embedding/\"+name+\"/\"+f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
