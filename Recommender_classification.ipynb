{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a1fe13f",
   "metadata": {},
   "source": [
    "<img src=\"https://digital.hbs.edu/platform-digit/wp-content/uploads/sites/2/2021/03/mercado-libre-logo-2.png\" alt=\"drawing\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfd5a8b",
   "metadata": {},
   "source": [
    "# [MEN-148](https://mercadolibre.atlassian.net/browse/MEN-148) - Mejoras RNN - VersiÃ³n 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c658d",
   "metadata": {},
   "source": [
    "## 2. Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b074870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estos primeros 3 installs son para la visualizar la arquitectura de la red neuronal\n",
    "# !pip install pydot\n",
    "# !pip install pydotplus\n",
    "# !apt-get install graphviz\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from commons.db import BigQueryConn\n",
    "import dask.dataframe as dd\n",
    "# from dask.dataframe import from_pandas\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "from sklearn.utils import class_weight\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "from commons.utils.timer import timer\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0041d",
   "metadata": {},
   "source": [
    "# Vuelvo con el approach anterior con compras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5560714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google_cloud\n",
    "import base64\n",
    "\n",
    "AUTH_BIGQUERY = os.environ[\"SECRET_MLD_MKETING_GCP_CRED\"]\n",
    "bq = google_cloud.BigQuery(AUTH_BIGQUERY)  #conectate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c6cbd932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results loaded to the table mld-marketing.clustering.ponspuhedt\n",
      "Exporto mld-marketing:clustering.ponspuhedt a gs://ext_ngrasso_fda/temp/training_rnn_next_item/*.parquet\n",
      "Deleted table 'mld-marketing.clustering.ponspuhedt'.\n"
     ]
    }
   ],
   "source": [
    "bq.fast_export(\"\"\"\n",
    "    WITH VISITAS AS(\n",
    "        SELECT a11.ORD_BUYER.ID as CUS_CUST_ID , sit_site_id, ORD_CREATED_DT as ord_date, ORD_CLOSED_DT as closed_dt\n",
    "        FROM `meli-bi-data.WHOWNER.BT_ORD_ORDERS` a11 \n",
    "        INNER JOIN mld-marketing.clustering.catalog_categories catalog\n",
    "            ON CAST(SUBSTR(catalog.category_id ,4)as INT64) = a11.ORD_CATEGORY.ID\n",
    "            AND a11.SIT_SITE_ID = catalog.site_id\n",
    "        INNER JOIN mld-marketing.clustering.catalog_categories_cleaned catalog_clean\n",
    "            ON catalog.domain_id = catalog_clean.domain_id\n",
    "            AND catalog.site_id = catalog_clean.site_id\n",
    "            AND catalog.domain_id IS NOT NULL \n",
    "        WHERE a11.ORD_STATUS = 'paid' AND a11.SIT_SITE_ID = 'MLA'\n",
    "            AND a11.ORD_CLOSED_DT in (date'2021-08-01',date'2021-07-31',date'2021-07-30')\n",
    "        GROUP BY 1,2,3,4\n",
    "    ),\n",
    "    \n",
    "    TRAIN_FEATURES AS(\n",
    "        SELECT  event.cus_cust_id, event.sit_site_id,closed_dt,\n",
    "                ARRAY_REVERSE(\n",
    "                    --ordena los 53 eventos mas recientes encontrados, con la fecha mas antigua primero\n",
    "                    ARRAY_AGG(\n",
    "                        --captura los 53 eventos mas recientes del usuario\n",
    "                        STRUCT(\n",
    "                            DATE_DIFF(DATE_ADD(ord_date, INTERVAL 1 DAY), event.log_date, DAY) as days_diff,\n",
    "                            catalog_clean.token as token,\n",
    "                            CASE WHEN \n",
    "                                (VIP_VIEW_APP_ANDROID_ITEM_COUNT + VIP_VIEW_APP_IOS_ITEM_COUNT + VIP_VIEW_DESKTOP_ITEM_COUNT + VIP_VIEW_WEB_MOBILE_ITEM_COUNT) > 5 THEN 3\n",
    "                            WHEN\n",
    "                                (VIP_VIEW_APP_ANDROID_ITEM_COUNT + VIP_VIEW_APP_IOS_ITEM_COUNT + VIP_VIEW_DESKTOP_ITEM_COUNT + VIP_VIEW_WEB_MOBILE_ITEM_COUNT) = 1 THEN 1\n",
    "                            ELSE\n",
    "                                2\n",
    "                            END as vips)\n",
    "                        ORDER BY event.log_date desc\n",
    "                        LIMIT 53\n",
    "                        )\n",
    "                ) as train_events\n",
    "        FROM meli-marketing.MODELLING.LK_EVENT_CUSTOMER_ITEMS event \n",
    "        INNER JOIN VISITAS v\n",
    "            ON v.CUS_CUST_ID = event.CUS_CUST_ID\n",
    "            AND v.SIT_SITE_ID = event.SIT_SITE_ID\n",
    "        INNER JOIN mld-marketing.clustering.catalog_categories catalog\n",
    "            ON CAST(SUBSTR(catalog.category_id ,4)as INT64) = event.cat_categ_id\n",
    "            AND event.SIT_SITE_ID = catalog.site_id\n",
    "            AND catalog.domain_id IS NOT NULL \n",
    "        INNER JOIN mld-marketing.clustering.catalog_categories_cleaned catalog_clean\n",
    "            ON catalog.domain_id = catalog_clean.domain_id\n",
    "            AND catalog.site_id = catalog_clean.site_id\n",
    "        WHERE  (VIP_VIEW_APP_ANDROID_ITEM_COUNT + VIP_VIEW_APP_IOS_ITEM_COUNT + VIP_VIEW_DESKTOP_ITEM_COUNT + VIP_VIEW_WEB_MOBILE_ITEM_COUNT) > 0\n",
    "                AND log_date < ord_date AND log_date >= DATE_SUB(ord_date, INTERVAL 60 DAY)\n",
    "        GROUP BY 1,2,3\n",
    "    ),\n",
    "    TARGET_FEATURES AS(\n",
    "        SELECT catalog_clean.token as target_token, a11.ORD_BUYER.ID as CUS_CUST_ID,\n",
    "                a11.sit_site_id,closed_dt\n",
    "        FROM `meli-bi-data.WHOWNER.BT_ORD_ORDERS` a11 \n",
    "        INNER JOIN VISITAS v\n",
    "            ON v.CUS_CUST_ID = a11.ORD_BUYER.ID\n",
    "            AND v.SIT_SITE_ID = a11.SIT_SITE_ID\n",
    "            AND a11.ORD_CLOSED_DT = v.closed_dt\n",
    "        INNER JOIN mld-marketing.clustering.catalog_categories catalog\n",
    "            ON CAST(SUBSTR(catalog.category_id ,4)as INT64) = a11.ORD_CATEGORY.ID\n",
    "            AND a11.SIT_SITE_ID = catalog.site_id\n",
    "        INNER JOIN mld-marketing.clustering.catalog_categories_cleaned catalog_clean\n",
    "            ON catalog.domain_id = catalog_clean.domain_id\n",
    "            AND catalog.site_id = catalog_clean.site_id\n",
    "            AND catalog.domain_id IS NOT NULL \n",
    "        WHERE a11.ORD_STATUS = 'paid' AND a11.SIT_SITE_ID = 'MLA'\n",
    "            \n",
    "        GROUP BY 1,2,3,4\n",
    "    )\n",
    "    SELECT train.cus_cust_id,train_events,target_token\n",
    "    FROM TRAIN_FEATURES train\n",
    "    INNER JOIN TARGET_FEATURES target\n",
    "        on target.cus_cust_id = train.cus_cust_id\n",
    "        AND target.sit_site_id = train.sit_site_id\n",
    "        AND train.closed_dt = target.closed_dt\n",
    "    where train.sit_site_id = 'MLA'\n",
    "        -- and MOD(train.cus_cust_id,100) < 4\n",
    "\"\"\",\"gs://ext_ngrasso_fda/temp/training_rnn_next_purchase\",temp_dataset=\"clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad82666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traemos la cantidad de dominios en MLA\n",
    "site_id=\"MLA\"\n",
    "clean_domains_query = f\"\"\"\n",
    "    SELECT  domain_id,token\n",
    "    FROM `mld-marketing.clustering.catalog_categories_cleaned` \n",
    "    WHERE site_id = '{site_id}'\n",
    "\"\"\"\n",
    "domains = bq.execute_response(clean_domains_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d0c285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files from storage and build a lazy dask dataframe to later get list of dicts\n",
    "destination = 'gs://ext_ngrasso_fda/temp/training_rnn_next_purchase/*.parquet' #todo MLA con token categories_cleaned, truncado 54 eventos\n",
    "df = dd.read_parquet(destination, storage_options={\"token\": json.loads(os.environ[\"SECRET_MLD_MKETING_GCP_CRED\"])}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18eb3070",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_amt = df.target_token.value_counts()\n",
    "df = df.loc[df.target_token.isin(domains_amt[domains_amt>=10].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c95c3925",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversor = df[[\"target_token\"]].drop_duplicates()\n",
    "conversor[\"token_categ\"] = conversor['target_token'].astype(\"category\").cat.codes + 1\n",
    "mapper = {x[\"target_token\"]:x[\"token_categ\"] for _,x in conversor.iterrows() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2e09ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "max_len = 53\n",
    "\n",
    "def two_steps(events):\n",
    "    \"\"\"\n",
    "    Separamos los dominios de cada usuario en forma de secuencia de caracteres separado por coma,\n",
    "    algo similar para los days_diff pero en forma de lista\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    days = []\n",
    "    vips = []  \n",
    "    append_tokens = tokens.append\n",
    "    append_days = days.append\n",
    "    append_vips = vips.append\n",
    "    \n",
    "    for event in events:\n",
    "        if(event['token'] in mapper):\n",
    "            append_tokens(mapper[event['token']])\n",
    "            append_days(event['days_diff'])\n",
    "            append_vips(event['vips'])\n",
    "    return [tokens, days,vips]\n",
    "\n",
    "#recuperamos los dominios y dias asociados a cada visita\n",
    "df['raw_sequence'] = df['train_events'].apply(lambda x: two_steps(x))\n",
    "df[\"input_domains\"] = df[\"raw_sequence\"].apply(lambda x: x[0])\n",
    "df[\"input_days\"] = df[\"raw_sequence\"].apply(lambda x: x[1])\n",
    "df[\"input_vips\"] = df[\"raw_sequence\"].apply(lambda x: x[2])\n",
    "\n",
    "df[\"n_domains\"] = df[\"input_domains\"].apply(lambda x: len(x))\n",
    "df = df.loc[df.n_domains > 4]\n",
    "\n",
    "del df['raw_sequence']\n",
    "\n",
    "df[\"input_domains\"] = df[\"input_domains\"].apply(lambda x: pad_sequences([x], maxlen=max_len)[0])\n",
    "df[\"input_days\"] = df[\"input_days\"].apply(lambda x: pad_sequences([x], maxlen=max_len)[0])\n",
    "df[\"input_vips\"] = df[\"input_vips\"].apply(lambda x: pad_sequences([x], maxlen=max_len)[0])\n",
    "\n",
    "df[\"target_token\"] = df[\"target_token\"].apply(lambda x: mapper[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e59fd299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_pickle(\"to_rnn.pkl\")\n",
    "# # !pip install google-cloud-core==1.5.0\n",
    "# import os\n",
    "# import google_cloud\n",
    "# import base64\n",
    "\n",
    "# AUTH_BIGQUERY = os.environ[\"SECRET_MLD_MKETING_GCP_CRED\"]\n",
    "# st = google_cloud.Storage(AUTH_BIGQUERY)  #conectate\n",
    "\n",
    "# st.upload_file(\"to_rnn.pkl\",\"gs://ext_ngrasso_fda/temp/training_dataset_sample_rnn_compras.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a58dfb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_pickle(\"to_rnn.pkl\")\n",
    "# !pip install google-cloud-core==1.5.0\n",
    "import os\n",
    "import google_cloud\n",
    "import base64\n",
    "\n",
    "AUTH_BIGQUERY = os.environ[\"SECRET_MLD_MKETING_GCP_CRED\"]\n",
    "st = google_cloud.Storage(AUTH_BIGQUERY)  #conectate\n",
    "\n",
    "st.downlad_file(\"gs://ext_ngrasso_fda/temp/training_dataset_sample_rnn_compras.pkl\",\"rnn.pkl\")\n",
    "df = pd.read_pickle(\"rnn.pkl\")\n",
    "\n",
    "max_len = 53\n",
    "vocab_size = df.target_token.nunique() + 1 #Â el 0 tambien cuenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9950a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting for train, validation\n",
    "df_train, _ = train_test_split(df, test_size = 0.4, random_state = 0, stratify=df.target_token)\n",
    "df_train, df_validation = train_test_split(df_train, test_size = 0.2, random_state = 0, stratify=df_train.target_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19455dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e6b5831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pasamos de numpy array a dict\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(df_train.target_token), df_train.target_token)\n",
    "weights = {i+1: class_weights[i] for i in range(len(class_weights))}\n",
    "weights[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08221f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 09:12:31.584092: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-02-08 09:12:31.584133: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-02-08 09:12:31.584161: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (5ffb8fcf4aa7): /proc/driver/nvidia/version does not exist\n",
      "2022-02-08 09:12:31.584657: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-08 09:12:31.614203: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2400000000 Hz\n",
      "2022-02-08 09:12:31.615807: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fed928bad0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-02-08 09:12:31.615831: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "# pasamos los datos de input a tensores para que el modelo pueda usarlo.\n",
    "X_seq_domains_train = tf.convert_to_tensor(list(df_train.input_domains))\n",
    "X_seq_days_train =  tf.convert_to_tensor(list(df_train.input_days))\n",
    "X_seq_vips_train =  tf.convert_to_tensor(list(df_train.input_vips))\n",
    "y_train_train = tf.convert_to_tensor(list(df_train.target_token))\n",
    "\n",
    "# X_seq_domains_test = tf.convert_to_tensor(list(df_test.input_domains))\n",
    "# X_seq_days_test =  tf.convert_to_tensor(list(df_test.input_days))\n",
    "# X_seq_vips_test =  tf.convert_to_tensor(list(df_test.input_vips))\n",
    "# y_train_test = tf.convert_to_tensor(list(df_test.target_token))\n",
    "\n",
    "X_seq_domains_validation = tf.convert_to_tensor(list(df_validation.input_domains))\n",
    "X_seq_days_validation =  tf.convert_to_tensor(list(df_validation.input_days))\n",
    "X_seq_vips_validation =  tf.convert_to_tensor(list(df_validation.input_vips))\n",
    "y_train_validation = tf.convert_to_tensor(list(df_validation.target_token))\n",
    "\n",
    "\n",
    "vocab_size_days = np.array(X_seq_days_train).max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "175d7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    We redefine our own loss function in order to get rid of the '0' value\n",
    "    which is the one used for padding. This to avoid that the model optimize itself\n",
    "    by predicting this value because it is the padding one.\n",
    "    \n",
    "    :param real: the truth\n",
    "    :param pred: predictions\n",
    "    :return: a masked loss where '0' in real (due to padding)\n",
    "                are not taken into account for the evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    # to check that pred is numric and not nan\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_object_ = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                                 reduction='none')\n",
    "    loss_ = loss_object_(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                   mode='min', \n",
    "                   patience=5, \n",
    "                   verbose=1, \n",
    "                   restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bdad150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1 = tf.keras.layers.Embedding(3, # numeros distintos\n",
    "#                           4, # dimension del vector\n",
    "#                           input_length=6 # cantidad de datos de entrada\n",
    "#                          )(np.array([[0,1,2,1,2,0],\n",
    "#                                     [0,1,2,1,2,0]]))\n",
    "# l2 = tf.constant(np.array([[10.,100.,1000.,10000.,100000.,100000.],\n",
    "#                           [10.,100.,1000.,10000.,100000.,100000.]]))\n",
    "# l2 = tf.keras.layers.Reshape((6,1))(l2)\n",
    "# tf.keras.layers.multiply([l1,l2]),l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b32eaf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "import tensorflow\n",
    "\n",
    "#reset Keras Session\n",
    "def reset_keras(model):\n",
    "    sess = tf.compat.v1.keras.backend.get_session()\n",
    "    tf.compat.v1.keras.backend.clear_session()\n",
    "    sess.close()\n",
    "    sess = tf.compat.v1.keras.backend.get_session()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a6fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "from keras import backend as K\n",
    "# from numba import cuda\n",
    "\n",
    "# me armo el set de testeo para ver novelty y covertura\n",
    "test_validation = df_validation.sample(frac=1.).drop_duplicates(subset=\"cus_cust_id\")\n",
    "X_seq_domains_test_val = tf.convert_to_tensor(list(test_validation.input_domains))\n",
    "X_seq_days_test_val =  tf.convert_to_tensor(list(test_validation.input_days))\n",
    "X_seq_vips_test_val =  tf.convert_to_tensor(list(test_validation.input_vips))\n",
    "\n",
    "\n",
    "def train_model_simil_mult(layers,nn1,lstm,lr,l1,l2,w_scaler = None,emb_domain=100,emb_days=100):\n",
    "    '''This is a model generating function so that we can search over neural net \n",
    "    parameters and architecture'''\n",
    "\n",
    "    drop_out = 0.5\n",
    "    reg = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
    "    adam = keras.optimizers.Adam(lr=lr)\n",
    "    \n",
    "    input_domain = keras.Input(shape=(X_seq_domains_train.shape[1],), name='input_dominios')\n",
    "    embedding_domain = keras.layers.Embedding(vocab_size, int(emb_domain), input_length=max_len)(input_domain)\n",
    "    \n",
    "    input_mult = keras.Input(shape=(X_seq_vips_train.shape[1],), name='input_dmultipliersVip')\n",
    "    input_T_mult = tf.keras.layers.Reshape((X_seq_vips_train.shape[1],1))(input_mult)\n",
    "    embedding_domain_mult = tf.keras.layers.multiply([embedding_domain,input_T_mult])\n",
    "    embedding_domain_mult = keras.layers.BatchNormalization(name='batchnorm_embedding')(embedding_domain_mult)\n",
    "    \n",
    "    encoding_padding_mask = tf.math.logical_not(tf.math.equal(input_domain, 0)) # para que no le de bola al padding\n",
    "    \n",
    "    input_days = keras.Input(shape=(X_seq_days_train.shape[1],),name='nb_days')    \n",
    "    embedding_days = keras.layers.Embedding(vocab_size_days, int(emb_days), input_length=max_len)(input_days)\n",
    "    \n",
    "    concat = keras.layers.concatenate([embedding_domain_mult, embedding_days], axis=-1,name='concat_embedding_input')\n",
    "    \n",
    "    rnn = keras.layers.LSTM(units=lstm,return_sequences=True,stateful=False,\n",
    "               recurrent_initializer='glorot_normal',name='LSTM_cat'\n",
    "              )(concat)\n",
    "    \n",
    "    rnn = keras.layers.BatchNormalization(name='batchnorm_lstm')(rnn)\n",
    "    \n",
    "    att = keras.layers.Attention(use_scale=False, causal=True,\n",
    "                                    name='attention')(inputs=[rnn, rnn],\n",
    "                                                      mask=[encoding_padding_mask,\n",
    "                                                            encoding_padding_mask])\n",
    "    \n",
    "    att_flatt = keras.layers.Flatten()(att)\n",
    "#     concat_features = keras.layers.concatenate([att_flatt, input_metadata,flatt_most_vist], axis=-1,name='concat_features')\n",
    "    dense1 = keras.layers.Dense(nn1,'relu',\n",
    "                                kernel_regularizer=reg ,\n",
    "                                name='dense1')(att_flatt)\n",
    "    dense1 = Dropout(drop_out)(dense1)\n",
    "    dense1 = keras.layers.BatchNormalization(name='batchnorm_lstm1')(dense1)\n",
    "    \n",
    "    if (layers == 2):\n",
    "        dense1 = keras.layers.Dense(nn2,'relu',\n",
    "                                kernel_regularizer=reg ,\n",
    "                                name='dense2')(dense1)\n",
    "        dense1 = Dropout(drop_out)(dense1)\n",
    "        dense1 = keras.layers.BatchNormalization(name='batchnorm_lstm2')(dense1)\n",
    "        \n",
    "    if (layers == 3):\n",
    "        dense1 = keras.layers.Dense(nn3,'relu',\n",
    "                            kernel_regularizer=reg ,\n",
    "                            name='dense3')(dense1)\n",
    "        dense1 = Dropout(drop_out)(dense1)\n",
    "        dense1 = keras.layers.BatchNormalization(name='batchnorm_lstm3')(dense1)\n",
    "        \n",
    "    output = tf.keras.layers.Dense(vocab_size, name='output', activation='softmax')(dense1)\n",
    "\n",
    "    model = keras.Model([input_domain,input_mult,input_days], output)\n",
    "\n",
    "    model.compile(loss=loss_function, optimizer=adam, metrics=[\"sparse_categorical_accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "layers = [1]\n",
    "nn1 = [3072,4096,5000,6144,7000,8000,9000]\n",
    "# nn2 = [100,512,1024,2048,3072,4096,5120]\n",
    "lstm = [50,75,100]\n",
    "w_scaler = [1.5,2,2.2,2.5,3,3.5,4,4.5,5,5.5,6,6.5,7]\n",
    "\n",
    "# dropout and regularisation\n",
    "l1 = [0.1,0.01, 0.001]\n",
    "l2 = [0, 0.1,0.01, 0.001]\n",
    "lr = [10**-3]\n",
    "\n",
    "\n",
    "# dictionary summary\n",
    "param_grid = dict(\n",
    "                    layers= layers,nn1=nn1,lstm=lstm, l1=l1, l2=l1, lr = lr,w_scaler = w_scaler              \n",
    "                 )\n",
    "\n",
    "i = 1\n",
    "for a in [len(x) for x in param_grid.values() if(type(x) != int)]: i=a*i\n",
    "print(\"Cantidad de parametros:\",i)\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "param_list = list(ParameterSampler(param_grid, n_iter=100,\n",
    "                                   random_state=rng))\n",
    "\n",
    "import csv  \n",
    "# import recmetrics\n",
    "\n",
    "l_hist = []\n",
    "for params in param_list:\n",
    "    print(params)\n",
    "    model = train_model_simil_mult(**params)\n",
    "    \n",
    "    weights = {i+1: class_weights[i]**params[\"w_scaler\"] for i in range(len(class_weights))}\n",
    "    weights[0] = 0\n",
    "\n",
    "    hist = model.fit([X_seq_domains_train,X_seq_vips_train,X_seq_days_train], y_train_train, \n",
    "                 validation_data = ([X_seq_domains_validation, X_seq_vips_validation,X_seq_days_validation], y_train_validation),\n",
    "                 epochs = 100, batch_size=512, callbacks=[early_stopping],\n",
    "                 class_weight = weights)\n",
    "\n",
    "    coverage = np.unique(np.argsort(-model.predict([X_seq_domains_test_val, X_seq_vips_test_val,X_seq_days_test_val]))[:,0:5]).shape[0]/3060\n",
    "\n",
    "    with open(\"rnn_train_metrics.csv\", 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([params,coverage,max(hist.history[\"val_sparse_categorical_accuracy\"])])\n",
    "        \n",
    "#     #limpio la memoria de la GPU para que corra todo ok\n",
    "#     K.clear_session()\n",
    "#     cuda.select_device(0)\n",
    "#     cuda.close()\n",
    "    \n",
    "    reset_keras(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
