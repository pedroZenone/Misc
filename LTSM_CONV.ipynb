{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from lifetimes.utils import summary_data_from_transaction_data\n",
    "import numpy as np\n",
    "import datetime\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "from lifetimes import ModifiedBetaGeoFitter\n",
    "from lifetimes import GammaGammaFitter\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import joblib\n",
    "import boto3\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "config = tf.ConfigProto(device_count={\"CPU\": 20})\n",
    "keras.backend.tensorflow_backend.set_session(tf.Session(config=config))\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.layers import Dropout\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from scipy.stats.distributions import expon\n",
    "from keras.models import load_model\n",
    "import io\n",
    "import json\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _write_dataframe_to_csv_on_s3(df, path_s3):\n",
    "    import boto3\n",
    "    from io import StringIO\n",
    "    \"\"\" Write a dataframe to a CSV on S3 \"\"\"\n",
    "    a = path_s3.split('//')\n",
    "    b = a[1].split('/')\n",
    "    bucket = b[0]\n",
    "    c = path_s3.split(bucket+'/')\n",
    "    path = c[1]\n",
    "    \n",
    "    buffer = StringIO()\n",
    "    df.to_csv(buffer,index=False)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Object(bucket, path).put(Body=buffer.getvalue())\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asigno_quintiles_6(x):\n",
    "    step = int(x.shape[0]*0.1) # segmentos de 10%\n",
    "    \n",
    "    l_ = []\n",
    "    for i in range(6):  # asigno los primeros 50% en segmentos de 10%\n",
    "        l_ = l_ + [\"q\"+str(i+1)]*step    \n",
    "    \n",
    "    step2 = x.shape[0] - len(l_)\n",
    "    l_ = l_ + [\"q6\"]*step2\n",
    "    return l_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug == True:\n",
    "    cores = 2\n",
    "    n_iter=2\n",
    "else:\n",
    "    cores = cpu_count()\n",
    "    n_iter = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(debug == False):\n",
    "    summary_cal = pd.read_csv('s3://fda-labs/ltv-ml/ML/preproc_data_train.csv.gzip')  # Because in the ETL we pickled the pandas dataframe\n",
    "else:\n",
    "    summary_cal = pd.read_csv('s3://fda-labs/ltv-ml/ML/sample_Preproc_MLA.csv')  # Because in the ETL we pickled the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cal = pd.merge(summary_cal,pd.read_csv(\"s3://fda-labs/ltv-ml/ML/summary_visitas_train.csv\",sep = \",\"),on=\"cust\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=50000) # le pongo mucha cantidad para que tome todas las catergorias ;)\n",
    "summary_cal[\"phrase\"] = summary_cal[\"phrase\"].fillna('')\n",
    "tokenizer.fit_on_texts(summary_cal[\"phrase\"])\n",
    "\n",
    "X_seq_train = tokenizer.texts_to_sequences(summary_cal[\"phrase\"])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "maxlen = 450\n",
    "X_seq_train = pad_sequences(X_seq_train, padding='pre', maxlen=maxlen)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols_struc = [ 'frequency', 'recency', 'T', 'monetary_value', 'my_recency', 'n_purchases_pred', 'p_alive',\n",
    "                       'montary_sales_forecast', 'clv_pareto', 'target_pareto','IPT_CV', 'money_sum',\n",
    "                       'money_CV', 'money_mean', 'money_std', 'money_max', 'money_min',\n",
    "                       'L1_1000', 'L1_1039', 'L1_1051', 'L1_1071', 'L1_1132', 'L1_1144',\n",
    "                       'L1_1168', 'L1_1182', 'L1_1246', 'L1_1276', 'L1_1367', 'L1_1368',\n",
    "                       'L1_1384', 'L1_1403', 'L1_1430', 'L1_1499', 'L1_1574', 'L1_1648',\n",
    "                       'L1_1798', 'L1_1953', 'L1_2547', 'L1_3025', 'L1_3937', 'L1_407134',\n",
    "                       'L1_409431', 'L1_5725', 'L1_5726', 'female', 'male', 'Account Money',\n",
    "                       'Bank Transfer', 'Credit Card', 'Debit Card', 'Ticket',\n",
    "                       'digital_currency', '01.Menor de 18 años', '02.Entre 18 y 25 años',\n",
    "                       '03.Entre 26 y 30 años', '04.Entre 31 y 35 años',\n",
    "                       '05.Entre 36 y 40 años', '06.Entre 41 y 55 años',\n",
    "                       '07.Entre 56 y 65 años', '08.Mayor de 65 años']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "Y_train = summary_cal[\"quintil_true\"].values\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_train)\n",
    "encoded_Y = encoder.transform(Y_train)\n",
    "y_train = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdc = StandardScaler()\n",
    "stdc_trained = stdc.fit(summary_cal[train_cols_struc])\n",
    "X_train_struc = stdc_trained.transform(summary_cal[train_cols_struc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mezclo loss datos para que no overfite!\n",
    "shuffle_index = np.random.choice(X_seq_train.shape[0], X_seq_train.shape[0], replace=False)\n",
    "X_seq_train = X_seq_train[shuffle_index,:]\n",
    "X_train_struc = X_train_struc[shuffle_index,:]\n",
    "y_train = y_train[shuffle_index,:]\n",
    "Y_train = Y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7326852b8209>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m weights = class_weight.compute_class_weight('balanced',\n\u001b[0;32m---> 38\u001b[0;31m                                                  np.unique(Y_train), Y_train)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_train' is not defined"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss',\n",
    "                   min_delta=0.01, \n",
    "                     patience=10, \n",
    "                   verbose=1, \n",
    "                   restore_best_weights=True)\n",
    "    \n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(Y_train), Y_train)\n",
    "\n",
    "weights = dict(zip(list(range(len(weights))), weights))\n",
    "\n",
    "def train_model(emb=32,conv=32,kernel = 3,nn=10, lr=0.01, decay=0., l1=0.01, l2=0.01,\n",
    "                 dropout=0,model_out = False):\n",
    "    print(emb,conv,kernel,nn,lr,decay,l1,l2,dropout)\n",
    "    '''This is a model generating function so that we can search over neural net \n",
    "    parameters and architecture'''\n",
    "\n",
    "    opt = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999,  decay=decay)\n",
    "    reg = keras.regularizers.l1_l2(l1=l1, l2=l2)\n",
    "    \n",
    "    # Rama visitas\n",
    "    input_visits = Input(shape=(X_seq_train.shape[1],))\n",
    "    # Define encoding layer\n",
    "    embedding = layers.Embedding(vocab_size, int(emb), input_length=maxlen)(input_visits)\n",
    "    next_seq = layers.Conv1D(int(conv), int(kernel), activation='relu')(embedding)\n",
    "    next_seq = layers.GlobalMaxPooling1D()(next_seq)\n",
    "    if(nn > 0):\n",
    "        next_seq = layers.Dense(int(nn), activation='relu', kernel_regularizer=reg)(next_seq)\n",
    "        if(dropout > 0):\n",
    "            next_seq =  layers.Dropout(dropout)(next_seq)\n",
    "        \n",
    "        next_seq = BatchNormalization()(next_seq)\n",
    "        \n",
    "    # Rama daatos estructurados    \n",
    "    input_structured = Input(shape=(X_train_struc.shape[1],))\n",
    "    next_dense =  layers.Dense(50, activation='relu', kernel_regularizer=reg)(input_structured)\n",
    "    if(dropout > 0):\n",
    "        next_dense =  layers.Dropout(dropout)(next_dense)\n",
    "    next_dense = BatchNormalization()(next_dense)\n",
    "    \n",
    "    next_dense =  layers.Dense(50, activation='relu', kernel_regularizer=reg)(next_dense)\n",
    "    if(dropout > 0):\n",
    "        next_dense =  layers.Dropout(dropout)(next_dense)\n",
    "    next_dense = BatchNormalization()(next_dense)\n",
    "    \n",
    "    concat = layers.concatenate([next_seq,next_dense], axis=-1)\n",
    "    out = layers.Dense(6, activation='softmax')(concat)\n",
    "\n",
    "    model = Model(inputs = [input_visits,input_structured], outputs = out)\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[f1])\n",
    "    \n",
    "    hist = model.fit([X_seq_train,X_train_struc],y_train,class_weight = weights, validation_split = 0.2,\n",
    "                 callbacks=[es],verbose=10,epochs = 50, batch_size=512)\n",
    "\n",
    "    if(model_out == True):\n",
    "        return model\n",
    "    \n",
    "    return hist\n",
    "\n",
    "def second_model(model_out):\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999,  decay=1e-9)\n",
    "    reg = keras.regularizers.l1_l2(l1=0.0001, l2=0.0001)\n",
    "    \n",
    "    # Rama visitas\n",
    "    input_visits = Input(shape=(X_seq_train.shape[1],))\n",
    "    # Define encoding layer\n",
    "    embedding = layers.Embedding(vocab_size, int(32), input_length=maxlen)(input_visits)\n",
    "    next_seq = layers.Conv1D(int(16), int(3), activation='relu')(embedding)\n",
    "    next_seq = layers.GlobalMaxPooling1D()(next_seq)\n",
    "\n",
    "    # Rama structurado\n",
    "    input_structured = Input(shape=(X_train_struc.shape[1],))\n",
    "    next_dense =  layers.Dense(100, activation='relu', kernel_regularizer=reg)(input_structured)\n",
    "    next_dense =  layers.Dropout(0.5)(next_dense)\n",
    "    next_dense = BatchNormalization()(next_dense)\n",
    "\n",
    "    next_dense =  layers.Dense(100, activation='relu', kernel_regularizer=reg)(input_structured)\n",
    "    next_dense =  layers.Dropout(0.5)(next_dense)\n",
    "    next_dense = BatchNormalization()(next_dense)\n",
    "    \n",
    "    next_dense =  layers.Dense(30, activation='relu', kernel_regularizer=reg)(input_structured)\n",
    "    next_dense =  layers.Dropout(0.5)(next_dense)\n",
    "    next_dense = BatchNormalization()(next_dense)\n",
    "    \n",
    "    concat = layers.concatenate([next_seq,next_dense], axis=-1)\n",
    "    out = layers.Dense(6, activation='softmax')(concat)\n",
    "\n",
    "    model = Model(inputs = [input_visits,input_structured], outputs = out)\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[f1])\n",
    "    \n",
    "    hist = model.fit([X_seq_train,X_train_struc],y_train,class_weight = weights, validation_split = 0.2,\n",
    "                 callbacks=[es],verbose=10,epochs = 50, batch_size=512)\n",
    "\n",
    "    if(model_out == True):\n",
    "        return model\n",
    "\n",
    "def third_model(model_out):\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999,  decay=1e-9)\n",
    "    reg = keras.regularizers.l1_l2(l1=0.0001, l2=0.0001)\n",
    "    \n",
    "    # Rama visitas\n",
    "    input_visits = Input(shape=(X_seq_train.shape[1],))\n",
    "    # Define encoding layer\n",
    "    embedding = layers.Embedding(vocab_size, int(32), input_length=maxlen)(input_visits)\n",
    "    next_seq = layers.Conv1D(int(16), int(3), activation='relu')(embedding)\n",
    "    next_seq = layers.GlobalMaxPooling1D()(next_seq)\n",
    "    next_seq = layers.Dense(int(100), activation='relu', kernel_regularizer=reg)(next_seq)\n",
    "    next_seq =  layers.Dropout(0.5)(next_seq)\n",
    "    next_seq = BatchNormalization()(next_seq)\n",
    "    next_seq = layers.Dense(int(30), activation='relu', kernel_regularizer=reg)(next_seq)\n",
    "    next_seq =  layers.Dropout(0.5)(next_seq)\n",
    "    next_seq = BatchNormalization()(next_seq)    \n",
    "    \n",
    "    # Rama structurado\n",
    "    input_structured = Input(shape=(X_train_struc.shape[1],))\n",
    "    next_dense =  layers.Dense(100, activation='relu', kernel_regularizer=reg)(input_structured)\n",
    "    next_dense =  layers.Dropout(0.5)(next_dense)\n",
    "    next_dense = BatchNormalization()(next_dense)\n",
    "\n",
    "    next_dense =  layers.Dense(100, activation='relu', kernel_regularizer=reg)(input_structured)\n",
    "    next_dense =  layers.Dropout(0.5)(next_dense)\n",
    "    next_dense = BatchNormalization()(next_dense)\n",
    "    \n",
    "    next_dense =  layers.Dense(30, activation='relu', kernel_regularizer=reg)(input_structured)\n",
    "    next_dense =  layers.Dropout(0.5)(next_dense)\n",
    "    next_dense = BatchNormalization()(next_dense)\n",
    "    \n",
    "    concat = layers.concatenate([next_seq,next_dense], axis=-1)\n",
    "    out = layers.Dense(6, activation='softmax')(concat)\n",
    "\n",
    "    model = Model(inputs = [input_visits,input_structured], outputs = out)\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[f1])\n",
    "    \n",
    "    hist = model.fit([X_seq_train,X_train_struc],y_train,class_weight = weights, validation_split = 0.2,\n",
    "                 callbacks=[es],verbose=10,epochs = 50, batch_size=512)\n",
    "\n",
    "    if(model_out == True):\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param = {'conv': 16.0,\n",
    " 'decay': 1e-09,\n",
    " 'dropout': 0.0,\n",
    " 'emb': 32.0,\n",
    " 'kernel': 3.0,\n",
    " 'l1': 0.01,\n",
    " 'l2': 0.001,\n",
    " 'lr': 0.01,\n",
    " 'nn': 100.0,\n",
    " 'model_out':True             \n",
    "}\n",
    "\n",
    "model = train_model(**best_param)\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(stdc_trained,\"std.sav\")\n",
    "\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with io.open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file(\"model.h5\", \"fda-labs\", \"ltv-ml/Embedding/model1.h5\")\n",
    "s3.upload_file(\"std.sav\", \"fda-labs\", \"ltv-ml/Embedding/std.sav\")\n",
    "s3.upload_file(\"tokenizer.json\", \"fda-labs\", \"ltv-ml/Embedding/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = second_model(True)\n",
    "model.save(\"model.h5\")\n",
    "s3.upload_file(\"model.h5\", \"fda-labs\", \"ltv-ml/Embedding/model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = third_model(True)\n",
    "model.save(\"model.h5\")\n",
    "s3.upload_file(\"model.h5\", \"fda-labs\", \"ltv-ml/Embedding/model3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de parametros: 25\n"
     ]
    }
   ],
   "source": [
    "# learning algorithm parameters\n",
    "lr=[1e-2, 1e-3]\n",
    "decay=[1e-6,1e-9,0]\n",
    "\n",
    "# numbers of layers\n",
    "nn = [0,10,100]\n",
    "emb = [10,32,50]\n",
    "conv = [8,16,32,64]\n",
    "kernel =[3,5] \n",
    "\n",
    "\n",
    "# dropout and regularisation\n",
    "dropout = [0,0.5]\n",
    "l1 = [0, 0.01, 0.001]\n",
    "l2 = [0, 0.01, 0.001]\n",
    "\n",
    "# dictionary summary\n",
    "param_grid = dict(\n",
    "                    emb = emb, conv = conv, kernel = kernel, nn=nn, lr=lr, decay=decay, l1=l1, \n",
    "                    l2=l2, dropout=dropout                  \n",
    "                 )\n",
    "\n",
    "print(\"Cantidad de parametros:\",sum([len(x) for x in param_grid.values() if(type(x) != int)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 32 5 0 0.01 1e-06 0.01 0 0\n",
      "Train on 204385 samples, validate on 51097 samples\n",
      "Epoch 1/2\n",
      "Epoch 2/2\n",
      "32 16 3 10 0.01 1e-06 0.001 0 0.5\n",
      "Train on 204385 samples, validate on 51097 samples\n",
      "Epoch 1/2\n",
      "Epoch 2/2\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "param_list = list(ParameterSampler(param_grid, n_iter=n_iter,\n",
    "                                   random_state=rng))\n",
    "\n",
    "l_hist = []\n",
    "for params in param_list:\n",
    "    params.update({\"score\":max(train_model(**params).history[\"val_f1\"])})\n",
    "    l_hist.append(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_45 (InputLayer)           (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_44 (InputLayer)           (None, 450)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_75 (Dense)                (None, 50)           3050        input_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_23 (Embedding)        (None, 450, 10)      106850      input_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 50)           200         dense_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 446, 32)      1632        embedding_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_76 (Dense)                (None, 50)           2550        batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 32)           0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 50)           200         dense_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 82)           0           global_max_pooling1d_22[0][0]    \n",
      "                                                                 batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_77 (Dense)                (None, 6)            498         concatenate_22[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 114,980\n",
      "Trainable params: 114,780\n",
      "Non-trainable params: 200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resu = pd.DataFrame(l_hist)\n",
    "_write_dataframe_to_csv_on_s3(resu,\"s3://fda-labs/ltv-ml/Embedding/modelPerformance.csv\")\n",
    "\n",
    "best_param = resu.loc[resu['score'].idxmax()].to_dict()\n",
    "print(best_param)\n",
    "best_param.pop('score', None)\n",
    "\n",
    "model = train_model(**best_param,model_out = True)\n",
    "model.save(\"model.h5\")\n",
    "\n",
    "joblib.dump(stdc_trained,\"std.sav\")\n",
    "\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with io.open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file(\"model.h5\", \"fda-labs\", \"ltv-ml/Embedding/model.h5\")\n",
    "s3.upload_file(\"std.sav\", \"fda-labs\", \"ltv-ml/Embedding/std.sav\")\n",
    "s3.upload_file(\"tokenizer.json\", \"fda-labs\", \"ltv-ml/Embedding/tokenizer.json\")\n",
    "\n",
    "# dependencies = {\n",
    "#     'f1': f1\n",
    "# }\n",
    "\n",
    "# load_model(\"model.h5\",custom_objects=dependencies).summary()\n",
    "\n",
    "\n",
    "# with open('tokenizer.json') as f:\n",
    "#     data = json.load(f)\n",
    "#     tokenizer = tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cal = pd.read_csv('s3://fda-labs/ltv-ml/ML/preproc_data_test.csv.gzip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cal = pd.merge(summary_cal,pd.read_csv(\"s3://fda-labs/ltv-ml/ML/summary_visitas_test.csv\",sep = \",\"),on=\"cust\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "s3.download_file(\"fda-labs\", \"ltv-ml/Embedding/std.sav\",\"std.sav\")\n",
    "stdc_trained = joblib.load(\"std.sav\")\n",
    "\n",
    "s3.download_file(\"fda-labs\", \"ltv-ml/Embedding/model1.h5\",\"model1.h5\")\n",
    "s3.download_file(\"fda-labs\", \"ltv-ml/Embedding/tokenizer.json\",\"tokenizer.json\")\n",
    "\n",
    "dependencies = {\n",
    "    'f1': f1\n",
    "}\n",
    "\n",
    "model = load_model(\"model1.h5\",custom_objects=dependencies)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(summary_cal.quintil_true)\n",
    "\n",
    "with open('tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cal[\"phrase\"] = summary_cal[\"phrase\"].fillna('')\n",
    "#tokenizer.fit_on_texts(summary_cal[\"phrase\"])\n",
    "X_seq_train = tokenizer.texts_to_sequences(summary_cal[\"phrase\"])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"paso\")\n",
    "maxlen = 450\n",
    "X_seq_train = pad_sequences(X_seq_train, padding='pre', maxlen=maxlen)      \n",
    "print(\"paso\")    \n",
    "X_train_struc = stdc_trained.transform(summary_cal[train_cols_struc]) \n",
    "y = np_utils.to_categorical(encoder.transform(summary_cal.quintil_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([X_seq_train,X_train_struc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cal[[\"q1\",\"q2\",\"q3\",\"q4\",\"q5\",\"q6\"]] = pd.DataFrame(preds)\n",
    "apply_me = summary_cal\n",
    "\n",
    "def asigno_quintiles_1(x,step,q):    \n",
    "    step2 = x.shape[0] - (step)\n",
    "    l_ = [\"q\"+q]*step + [\"non_q\"]*step2\n",
    "    return l_ \n",
    "\n",
    "apply_me[\"pred_q\"] = \"non_q\"\n",
    "resu = pd.DataFrame([])\n",
    "step1 = int(apply_me.shape[0]*0.1)\n",
    "step2 = int(apply_me.shape[0]*0.5)\n",
    "\n",
    "for i in range(4):\n",
    "    df = apply_me.loc[apply_me.pred_q == \"non_q\"]\n",
    "    df = df.sort_values(by = \"q\"+str(i+1),ascending = False)\n",
    "    df[\"pred_q\"] = asigno_quintiles_1(df,step1,str(i+1))\n",
    "    df = df.loc[df.pred_q != \"non_q\"]\n",
    "    apply_me.loc[apply_me.cust.isin(df.cust.values),\"pred_q\"] = \"q\"+str(i+1)\n",
    "\n",
    "df = apply_me.loc[apply_me.pred_q == \"non_q\"]\n",
    "df = df.sort_values(by = \"q6\",ascending = False)\n",
    "df[\"pred_q\"] = asigno_quintiles_1(df,step2,\"6\")\n",
    "df = df.loc[df.pred_q != \"non_q\"]\n",
    "apply_me.loc[apply_me.cust.isin(df.cust.values),\"pred_q\"] = \"q6\"\n",
    "\n",
    "apply_me.loc[apply_me.pred_q == \"non_q\",\"pred_q\"] = \"q5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(apply_me[\"quintil_true\"],apply_me[\"pred_q\"])\n",
    "for i in range(confusion.shape[0]):\n",
    "    confusion[i,:] = 100*confusion[i,:]/confusion.sum(axis = 1)[i]\n",
    "    \n",
    "labels = ['q1', 'q2','q3','q4','q5','q6']\n",
    "plt.figure(figsize=(10,10))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(confusion,annot = True,center = 1,ax=ax,cmap='bwr')\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix Modelo Dense'); \n",
    "ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# import pydotplus\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "# import pydot\n",
    "\n",
    "# keras.utils.vis_utils.pydot = pydot\n",
    "\n",
    "# plot_model(net, show_shapes=True, to_file='model.png')\n",
    "# # apt-get install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-04 23:35:05   13781183 doc2vec_10_12\n",
      "2019-07-04 20:08:17   15115133 doc2vec_20_8\n",
      "2019-07-05 03:06:43   16721256 doc2vec_32_12\n",
      "2019-07-05 06:36:22   16734868 doc2vec_32_8\n",
      "2019-07-03 15:15:46      66958 model.png\n",
      "2019-07-05 14:27:12    4496000 model1.h5\n",
      "2019-07-05 14:38:53    4396856 model2.h5\n",
      "2019-07-05 14:55:57    4482408 model3.h5\n",
      "2019-07-04 16:31:31        711 modelPerformance.csv\n",
      "2019-07-05 14:27:12       2014 std.sav\n",
      "2019-07-05 14:27:12    1260216 tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://fda-labs/ltv-ml/Embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['q1', 'q2','q3','q4','q5','q6']\n",
    "plt.figure(figsize=(10,10))\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(confusion,annot = True,center = 1,ax=ax,cmap='bwr')\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix Modelo Dense'); \n",
    "ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n",
    "\n",
    "plt.savefig(\"confusion_matrix.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://fda-labs/ltv-ml/Dense/ModelPerformance.csv to ./read.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://fda-labs/ltv-ml/Dense/ModelPerformance.csv read.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cust</th>\n",
       "      <th>frequency</th>\n",
       "      <th>recency</th>\n",
       "      <th>T</th>\n",
       "      <th>monetary_value</th>\n",
       "      <th>date_min</th>\n",
       "      <th>date_max</th>\n",
       "      <th>my_recency</th>\n",
       "      <th>n_purchases_pred</th>\n",
       "      <th>p_alive</th>\n",
       "      <th>...</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>digital_currency</th>\n",
       "      <th>01.Menor de 18 años</th>\n",
       "      <th>02.Entre 18 y 25 años</th>\n",
       "      <th>03.Entre 26 y 30 años</th>\n",
       "      <th>04.Entre 31 y 35 años</th>\n",
       "      <th>05.Entre 36 y 40 años</th>\n",
       "      <th>06.Entre 41 y 55 años</th>\n",
       "      <th>07.Entre 56 y 65 años</th>\n",
       "      <th>08.Mayor de 65 años</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137310278</td>\n",
       "      <td>77.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2608.509221</td>\n",
       "      <td>2016-12-20</td>\n",
       "      <td>2018-06-20</td>\n",
       "      <td>0</td>\n",
       "      <td>288.677877</td>\n",
       "      <td>0.998795</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37441540</td>\n",
       "      <td>75.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1624.304133</td>\n",
       "      <td>2016-12-20</td>\n",
       "      <td>2018-06-20</td>\n",
       "      <td>0</td>\n",
       "      <td>281.264793</td>\n",
       "      <td>0.998764</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84109342</td>\n",
       "      <td>66.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1982.590000</td>\n",
       "      <td>2016-12-29</td>\n",
       "      <td>2018-06-19</td>\n",
       "      <td>0</td>\n",
       "      <td>250.556642</td>\n",
       "      <td>0.998598</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>127383407</td>\n",
       "      <td>77.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1404.966753</td>\n",
       "      <td>2016-12-22</td>\n",
       "      <td>2018-06-20</td>\n",
       "      <td>0</td>\n",
       "      <td>288.677877</td>\n",
       "      <td>0.998795</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178090012</td>\n",
       "      <td>71.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1431.546620</td>\n",
       "      <td>2016-12-26</td>\n",
       "      <td>2018-06-18</td>\n",
       "      <td>0</td>\n",
       "      <td>269.287501</td>\n",
       "      <td>0.998695</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cust  frequency  recency     T  monetary_value    date_min  \\\n",
       "0  137310278       77.0     78.0  78.0     2608.509221  2016-12-20   \n",
       "1   37441540       75.0     78.0  78.0     1624.304133  2016-12-20   \n",
       "2   84109342       66.0     77.0  77.0     1982.590000  2016-12-29   \n",
       "3  127383407       77.0     78.0  78.0     1404.966753  2016-12-22   \n",
       "4  178090012       71.0     77.0  77.0     1431.546620  2016-12-26   \n",
       "\n",
       "     date_max  my_recency  n_purchases_pred   p_alive  ...  Ticket  \\\n",
       "0  2018-06-20           0        288.677877  0.998795  ...       0   \n",
       "1  2018-06-20           0        281.264793  0.998764  ...       0   \n",
       "2  2018-06-19           0        250.556642  0.998598  ...       0   \n",
       "3  2018-06-20           0        288.677877  0.998795  ...       0   \n",
       "4  2018-06-18           0        269.287501  0.998695  ...       0   \n",
       "\n",
       "   digital_currency  01.Menor de 18 años  02.Entre 18 y 25 años  \\\n",
       "0                 0                    0                      0   \n",
       "1                 0                    0                      0   \n",
       "2                 0                    0                      0   \n",
       "3                 0                    0                      0   \n",
       "4                 0                    0                      0   \n",
       "\n",
       "   03.Entre 26 y 30 años  04.Entre 31 y 35 años  05.Entre 36 y 40 años  \\\n",
       "0                      0                      0                      0   \n",
       "1                      0                      1                      0   \n",
       "2                      0                      0                      0   \n",
       "3                      0                      0                      0   \n",
       "4                      0                      0                      0   \n",
       "\n",
       "   06.Entre 41 y 55 años  07.Entre 56 y 65 años  08.Mayor de 65 años  \n",
       "0                      1                      0                    0  \n",
       "1                      0                      0                    0  \n",
       "2                      1                      0                    0  \n",
       "3                      0                      1                    0  \n",
       "4                      0                      0                    0  \n",
       "\n",
       "[5 rows x 97 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_cal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
