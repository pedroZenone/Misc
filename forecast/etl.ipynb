{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting\") # en demograficos tengo el GENDER\n",
    "import os\n",
    "#import dask.dataframe as dd\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import boto3\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error,average_precision_score\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "from bayes_opt import BayesianOptimization\n",
    "#import lightgbm as lgb\n",
    "import warnings\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from multiprocessing import  cpu_count,Pool\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from lifetimes import ModifiedBetaGeoFitter\n",
    "from lifetimes import GammaGammaFitter\n",
    "from dateutil.relativedelta import relativedelta \n",
    "from lifetimes.utils import summary_data_from_transaction_data\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "\n",
    "import gc\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import pyarrow\n",
    "\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.expanduser(\".\")))\n",
    "sys.path.append(os.path.dirname(os.path.expanduser(\"..\")))\n",
    "sys.path.append(os.path.dirname(os.path.expanduser(\"../defines\")))\n",
    "sys.path.append(os.path.dirname(os.path.expanduser(\"./utils.py\")))\n",
    "from defines import *\n",
    "\n",
    "TARGET_COL = \"frequency_eval\"\n",
    "TRAIN = True\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "from etl_functions import *\n",
    "from preprocessing import *\n",
    "from drift_utils import *\n",
    "from train_utils import train_combinations,simple_fit,foo_evaluation_regression, foo_model_regression,foo_predict_regression,anti_dummies,caster,picture\n",
    "\n",
    "MODEL_FILES = ['ASP.pkl','clusters.pkl','model_0.pkl','model_1.pkl','model_2.pkl','model_3.pkl',\n",
    "           'features_model_0.csv','features_model_1.csv','features_model_2.csv','features_model_3.csv']  \n",
    "\n",
    "EXCLUDE_VARIABLES_DRIFTED = {0:[],1:[],2:[],3:[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ['SECRET_DEBUG'] == 'True':\n",
    "    print(\"DEBUG: True\")\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cliente BigQuery\n",
    "\n",
    "from google_cloud import BigQuery\n",
    "from google_cloud import Storage\n",
    "import base64\n",
    "import os\n",
    "bq = BigQuery(AUTH_BIGQUERY)\n",
    "bqs = Storage(AUTH_BIGQUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generacion de fechas:\n",
    "# 2 dias random en el periodo from_ : to_ por cada dia de la semana, para procurar un\n",
    "# balance de transacciones salvando las diferencias que hay en compras entre dias habiles\n",
    "# y fines de semana\n",
    "\n",
    "to_ = FECHA_END - relativedelta(months=FORECAST_MONTHS) - relativedelta(days=1)\n",
    "from_ = to_ - relativedelta(months = 2)\n",
    "\n",
    "fechas_queries = [] \n",
    "\n",
    "for j in range(7):\n",
    "    dias = np.random.choice(list(range(8)),size=2,replace = False)\n",
    "    aux = from_ + relativedelta(weeks = int(dias[0]))\n",
    "    aux += relativedelta(days=1+j) \n",
    "    aux += relativedelta(months=FORECAST_MONTHS)  \n",
    "    fechas_queries.append(aux)\n",
    "    aux = from_ + relativedelta(weeks = int(dias[1]))\n",
    "    aux += relativedelta(days=1+j) \n",
    "    aux += relativedelta(months=FORECAST_MONTHS)  \n",
    "    fechas_queries.append(aux)\n",
    "\n",
    "pd.DataFrame([x.strftime(\"%Y_%m_%d\") for x in fechas_queries],columns = [\"Fecha\"]).to_csv(\"fechas.csv\",index=False,sep=\"|\")\n",
    "bqs.upload_file('fechas.csv',gcps_path_in + \"fechas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fechas_queries = pd.read_csv(\"fechas.csv\")\n",
    "fechas_queries = [datetime.datetime.strptime(x,\"%Y_%m_%d\" ) for x in fechas_queries.Fecha.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_queries(bq,f,n_sample = 300000):\n",
    "    # Funcion para correr todas las queries \n",
    "    bq.execute(f\"DROP TABLE IF EXISTS {tabla_pivot2bq}\")\n",
    "    end_s = f.strftime(\"%Y_%m_%d\")\n",
    "    gcps_path_in_ = gcps_path_in + end_s + \"/\"\n",
    "    today = f\n",
    "    end_cal_test = f\n",
    "    start_cal_test = end_cal_test - relativedelta(months=FORECAST_MONTHS)\n",
    "    end_cal_train = start_cal_test - relativedelta(days=1)\n",
    "    start_cal_train = end_cal_train - relativedelta(months=TRAINING_MONTHS)\n",
    "\n",
    "    ETL_TABLES = []\n",
    "\n",
    "    etl_full_table( gcps_path_in = gcps_path_in_, bq_app = bq, date_to=end_cal_train, date_from=start_cal_train, sample=True, n_sample = n_sample)\n",
    "    size = bq.execute_response(f\"SELECT COUNT(*) as size FROM {tabla_pivot2bq}\")['size'][0]\n",
    "    print(f\"Size:{size}\")\n",
    "    \n",
    "    # Target corre con start_cal_test y end_cal_test\n",
    "    etl_target(gcps_path_in = gcps_path_in_, bq_app = bq,date_from = start_cal_test, date_to = end_cal_test, tables = ETL_TABLES)\n",
    "\n",
    "\n",
    "    # Funciones que corren con queries para cada campo\n",
    "    functions = [etl_IPT,etl_sellers,etl_orders_frequencies,etl_asp_pareto,etl_compras_shipping,etl_payments,etl_tarjetas,\n",
    "                etl_locations,etl_demo,etl_installs,etl_visits]\n",
    "\n",
    "    if(PAIS in [\"MLA\",\"MLB\"]):        \n",
    "        functions = functions + [etl_mp_app,etl_mp_mgm,etl_mp_prepaid]\n",
    "\n",
    "    if(PAIS in [\"MLB\"]):\n",
    "        functions = functions + [etl_reg_queries]\n",
    "\n",
    "    for func in functions:\n",
    "        func(gcps_path_in = gcps_path_in_, bq_app = bq,date_from = start_cal_train, date_to = end_cal_train, tables = ETL_TABLES)\n",
    "\n",
    "    # Query autocontenida para realizar todos los join\n",
    "    base_q = \"\"\"SELECT PIVOT.CUS_CUST_ID,\\n\"\"\"\n",
    "    base_q = (base_q + \"\\n\".join(f\"\"\"{t}.* EXCEPT (CUS_CUST_ID),\"\"\" for t in ETL_TABLES))[0:-1] #[0:-2] borra la ultima coma\n",
    "    base_q = base_q + f\"\"\"\\nFROM {tabla_pivot2bq} PIVOT\\n\"\"\"\n",
    "    base_q = (base_q + \"\\n\".join(f\"\"\"LEFT JOIN {temp45_base_path + t} {t} \\n\\tON {t}.CUS_CUST_ID = PIVOT.CUS_CUST_ID\"\"\" for t in ETL_TABLES))\n",
    "    create_q = f\"\"\"CREATE TABLE IF NOT EXISTS {temp45_base_path + 'SUMMARY_CAL_' + end_s} AS (\\n{base_q}\\n)\"\"\"\n",
    "    bq.execute(create_q)\n",
    "    print(ETL_TABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tables(bq,fechas_queries,storage_folder):\n",
    "    # Union de todas las tablas generadas por sampleo en una unica tabla de entrenamiento\n",
    "\n",
    "    base_q = \"\"\"SELECT * FROM\\n\"\"\"\n",
    "    union_q = \"\"\"\\nUNION ALL\\n\"\"\".join([f\"SELECT * FROM {temp45_base_path + 'SUMMARY_CAL_' + t.strftime('%Y_%m_%d')}\" for t in fechas_queries])\n",
    "    print(union_q)\n",
    "    bq.fast_export(union_q,gcps_path_in + storage_folder)\n",
    "\n",
    "    # Borrar tablas parciales generadas en la ultima parte del loop\n",
    "    last_tables = list(bq.execute_response(f\"\"\"SELECT * FROM meli-marketing.TEMP45.INFORMATION_SCHEMA.TABLES\n",
    "                    WHERE table_name LIKE '%{temp45_base_path.split('.')[-1]}%'\"\"\" )['table_name'])\n",
    "\n",
    "    for table in last_tables:\n",
    "        bq.execute(f\"DROP TABLE IF EXISTS meli-marketing.TEMP45.{table}\")\n",
    "\n",
    "    # Borrar tablas summary generadas para este pais\n",
    "    summary_tables = list(bq.execute_response(f\"\"\"SELECT * FROM meli-marketing.TEMP45.INFORMATION_SCHEMA.TABLES\n",
    "                        WHERE table_name LIKE '%{PAIS}_ML_FORECAST_3M_SUMMARY_CAL_%'\"\"\" )['table_name'])\n",
    "\n",
    "    for table in summary_tables:\n",
    "        bq.execute(f\"DROP TABLE IF EXISTS meli-marketing.TEMP45.{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.monotonic()\n",
    "#     for f in fechas_queries:\n",
    "for f in fechas_queries:\n",
    "    run_queries(bq,f)\n",
    "\n",
    "    utils.prints3(\"Ready \",f)\n",
    "join_tables(bq,fechas_queries,'summary_cal')\n",
    "\n",
    "finish_time = (time.monotonic() - start_time)/60\n",
    "print(f'ETL Time: {finish_time} minutes')\n",
    "bqs.print_gcps(\"---------f'ETL Time: {finish_time} minutes'-----------\",bq_log_path=bq_log_path,silent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drift detection dataset generation\n",
    "# to_ = FECHA_END - relativedelta(months=FORECAST_MONTHS) - relativedelta(days=1)\n",
    "# from_ = to_ - relativedelta(months = 2)\n",
    "\n",
    "date_drift_check = FECHA_END\n",
    "run_queries(bq,date_drift_check,n_sample = 1000000)\n",
    "table = f\"\"\"{temp45_base_path + 'SUMMARY_CAL_' + date_drift_check.strftime(\"%Y_%m_%d\")}\"\"\"\n",
    "bq.fast_export(f\"\"\"SELECT * FROM {table}\"\"\",\n",
    "               gcps_path_in + \"summary_cal_drift\")\n",
    "bq.execute(f\"DROP TABLE IF EXISTS {table}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(gcps_path_in, folder_path):    \n",
    "    \n",
    "    summary_cal = read_gfs_parquet(gcps_path_in, bqs, folder_path)\n",
    "    \n",
    "    # Steps para preprocesar\n",
    "    steps = []\n",
    "    if (PAIS == 'MLB' or PAIS == 'MLA'):\n",
    "        steps = steps + [prepaid,asset_mgm,install_mp]\n",
    "    if (PAIS == 'MLB'):\n",
    "        steps = steps + [reg_data]\n",
    "    steps = steps + [orders_frequencies,IPT,locations,demograficos, compras_shipping, target, tarjetas, visitas_short,\n",
    "         install_ml, payments, sellers, asp_pareto]\n",
    "\n",
    "    # Traigo base de clientes como pivot para añadir todas las columnas\n",
    "\n",
    "    \n",
    "    for func in steps:\n",
    "        start = time.time()\n",
    "        bqs.print_gcps(\"--------------------\",bq_log_path=bq_log_path,silent = True)\n",
    "        bqs.print_gcps(\"Start time:\",datetime.datetime.now(),bq_log_path=bq_log_path,silent = True)\n",
    "        bqs.print_gcps(\"Starting:\",func.__name__,\"Summary size:\",summary_cal.shape,bq_log_path=bq_log_path)\n",
    "        summary_cal = func(bqs, gcps_path_in,summary_cal,\"train\")\n",
    "        end = time.time()\n",
    "        bqs.print_gcps(\"tiempo en resolver:\", end-start,bq_log_path=bq_log_path,silent = True)\n",
    "    \n",
    "    summary_cal = generate_clusters(bqs,gcps_path_in,summary_cal,\"train_train\")\n",
    "    summary_cal = pareto(bqs,gcps_path_in,summary_cal,\"train_train\") # Genero modelo forecast\n",
    "    \n",
    "    return summary_cal.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set fechas para descargar y joinear los datasets de entrenamiento\n",
    "\n",
    "bqs.download_file(gcps_path_in + 'fechas.csv', 'fechas.csv')\n",
    "fechas_queries = pd.read_csv('fechas.csv')\n",
    "fechas_queries = [x for x in fechas_queries.Fecha.values]\n",
    "summary_cal = preprocess(gcps_path_in, \"summary_cal\")\n",
    "\n",
    "\n",
    "train_cols_by_segment = {0:[x for x in summary_cal.columns if x not in  ['CUS_CUST_ID', TARGET_COL,'GMV_TARGET']],\n",
    "                    1:[x for x in summary_cal.columns if x not in  ['CUS_CUST_ID', TARGET_COL,'GMV_TARGET']],\n",
    "                    2:[x for x in summary_cal.columns if x not in  ['CUS_CUST_ID', TARGET_COL,'GMV_TARGET']],\n",
    "                    3:[x for x in summary_cal.columns if x not in  ['CUS_CUST_ID', TARGET_COL,'GMV_TARGET']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo las variables drifteadas desde el dataset de entrenamiento (Today - 3 months) y el dataset\n",
    "# del dia de hoy, para excluirlas del entrenamiento\n",
    "\n",
    "summary_drift = preprocess(gcps_path_in, \"summary_cal_drift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cal_train_sample = {}\n",
    "summary_cal_today_sample = {}\n",
    "model_drifts = {}\n",
    "\n",
    "\n",
    "for x in range(N_CLUSTERS):\n",
    "    summary_cal_train_sample[x] = summary_cal[summary_cal.cluster == x].sample(15000, random_state = RANDOM_STATE)\n",
    "    summary_cal_today_sample[x] = summary_drift[summary_drift.cluster == x].sample(15000, random_state = RANDOM_STATE)\n",
    "    \n",
    "    model_drifts[x] = Drift(setpoint_df = summary_cal_train_sample[x],\n",
    "                            apply_df = summary_cal_today_sample[x],\n",
    "                            target = TARGET_COL)\n",
    "    model_drifts[x].apply_drift()\n",
    "    EXCLUDE_VARIABLES_DRIFTED[x] = model_drifts[x].getDriftedTotal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparo_datos(summary_cal):\n",
    "    \n",
    "    numeric_dtypes = [np.dtype('float32'),np.dtype('float64'), np.dtype('int32'),np.dtype('int64')]\n",
    "\n",
    "    total_cols = list(summary_cal.columns)\n",
    "    if('CUS_CUST_ID' in total_cols): total_cols.remove('CUS_CUST_ID')\n",
    "    \n",
    "    # Evitar filtrar la variable target\n",
    "    if(TARGET_COL in total_cols): total_cols.remove(TARGET_COL)\n",
    "    \n",
    "    # Eliminado de la query de Target, pero si lo vuelven a usar esta \n",
    "    if('GMV_TARGET' in total_cols): total_cols.remove('GMV_TARGET')\n",
    "    \n",
    "    \n",
    "    train_cols_nocat = [x for x in total_cols if(summary_cal.dtypes[x] in numeric_dtypes)]\n",
    "    \n",
    "    cates = [x for x in total_cols if(summary_cal.dtypes[x] not in numeric_dtypes)]\n",
    "\n",
    "    for x in train_cols_nocat:\n",
    "        summary_cal[x] = summary_cal[x].astype(int)\n",
    "    for x in cates:\n",
    "        summary_cal[x] = summary_cal[x].astype(str)\n",
    "    \n",
    "    \n",
    "    return summary_cal,cates,train_cols_nocat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_model(necessary_files,bqs):\n",
    "    import re\n",
    "    matches = {}\n",
    "    bqs.create_clients()\n",
    "    gs_bucket = bqs.gsClient.get_bucket('marketing-modelling') # me paro en mi bucket\n",
    "    paths = gs_bucket.list_blobs(prefix=\"\")\n",
    "    for blob in paths:\n",
    "        # Check si el pais esta y todos los archivos necesarios estan en esa ruta\n",
    "        if ((f\"{BU}/{INICIATIVA}/{PAIS}/model\" in blob.name)): \n",
    "            match = re.search(r'\\d{4}_\\d{2}_\\d{2}', blob.name).group()\n",
    "            \n",
    "            #Si contiene alguno de los archivos necesarios agrego la fecha al diccionario de cuenta de archivos\n",
    "            if (any(file in blob.name for file in neccesary_files)):\n",
    "                if match not in matches.keys():\n",
    "                    matches[match] = 1\n",
    "                else:\n",
    "                    matches[match] = matches[match] + 1\n",
    "\n",
    "    #print(matches)\n",
    "\n",
    "    matches_all_files = [match for match in matches.keys() if (matches[match] == len(neccesary_files))]\n",
    "    dates = [datetime.datetime.strptime(x,\"%Y_%m_%d\") for x in matches_all_files]\n",
    "    if dates == []:\n",
    "        latest_date = None\n",
    "        latest_date_str = None\n",
    "    else:\n",
    "        latest_date = max(dates)\n",
    "        latest_date_str = latest_date.strftime(\"%Y_%m_%d\")\n",
    "\n",
    "\n",
    "    return latest_date_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hago seleccion de variables imporantes por segmento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entreno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_by_segmento(summary_cal,l_train_cols, n_cluster, select_variables = False):\n",
    "    summary_cal,cates,train_cols_nocat = preparo_datos(summary_cal[l_train_cols])\n",
    "    #train, valid= train_test_split(summary_cal,test_size=0.15,random_state=0)\n",
    "    train, valid= train_test_split(summary_cal[l_train_cols],test_size=0.15,random_state=0)\n",
    "    train_full, test= train_test_split(train,test_size=0.2,random_state=0)\n",
    "    \n",
    "    if (select_variables):\n",
    "        if(train_full.shape[0] > 1e6):\n",
    "            train, _= train_test_split(train_full,test_size=0.6,random_state=0)  # reduzco para entrenamieto\n",
    "        else:\n",
    "            train = train_full\n",
    "    else:\n",
    "        if(train_full.shape[0] > 600000):\n",
    "            train, _= train_test_split(train_full,train_size=500000,random_state=0)  # reduzco para entrenamieto\n",
    "            print(\"resampleo de\",train_full.shape,\"a\",train.shape[0])\n",
    "\n",
    "        else:\n",
    "            train = train_full\n",
    "        \n",
    "    print(\"Train:\",train.shape[0],\"Test:\",test.shape[0],\"Valid;\",valid.shape[0])\n",
    "    X_train_ =  np.c_[train[train_cols_nocat].values,train[cates].values]\n",
    "    X_train_full =  np.c_[train_full[train_cols_nocat].values,train_full[cates].values]\n",
    "    X_valid_ =  np.c_[valid[train_cols_nocat].values,valid[cates].values]\n",
    "    X_test =  np.c_[test[train_cols_nocat].values,test[cates].values]\n",
    "\n",
    "    y_train_full =  np.array(train_full[TARGET_COL])\n",
    "    y_train_ =  np.array(train[TARGET_COL])\n",
    "    y_valid_ =  np.array(valid[TARGET_COL])\n",
    "    y_test =  np.array(test[TARGET_COL])  \n",
    "\n",
    "    cates_ind = [X_train_.shape[1] - len(cates) + i for i in range(len(cates))]  # indices de las categorias\n",
    "\n",
    "    # Entreno    \n",
    "    basic_params = {\n",
    "        \n",
    "               'use_best_model':True,\n",
    "               'loss_function': 'MAE',\n",
    "                'eval_metric':'MAE',\n",
    "                'verbose':False,\n",
    "                'boosting_type':\"Plain\",\n",
    "                'bootstrap_type':\"Bernoulli\",\n",
    "                'iterations':500,\n",
    "    #              'class_weights':weights_l,\n",
    "    #              'rsm':0.1,  # OJO!!! esto solo va si tenes mas de 100 features!,\n",
    "    #             'max_ctr_complexity':2,\n",
    "               'random_state': 45\n",
    "            }\n",
    "\n",
    "\n",
    "    \n",
    "    hyperparams = { \n",
    "                    'learning_rate':[0.01,0.05,0.1,0.2,0.3],\n",
    "                    'depth': [1,3,5,7,10],                                                       \n",
    "                    'l2_leaf_reg': [1,10,50,100,150,300,500,750,1000],\n",
    "                    'border_count': [5,10,30,60,100,150,200]\n",
    "                }\n",
    "\n",
    "    d_types = { 'depth': int,\n",
    "               'l2_leaf_reg': int,\n",
    "               'border_count':int\n",
    "              }\n",
    "\n",
    "    if select_variables:\n",
    "        \n",
    "        params = train_combinations(X_train_,X_valid_,y_train_,y_valid_,\"neg_mean_absolute_error\",foo_predict_regression,\n",
    "                              foo_model_regression,basic_params,d_types,hyperparams,cates_ind, n_iter = SELECT_VARIABLES_STEPS)\n",
    "        best_model =  simple_fit(params,X_train_full,y_train_full,X_valid_,y_valid_,foo_model_regression,basic_params,d_types,cates_ind)\n",
    "    \n",
    "    else:\n",
    "        params = train_combinations(X_train_,X_valid_,y_train_,y_valid_,\"neg_mean_absolute_error\",foo_predict_regression,\n",
    "                          foo_model_regression,basic_params,d_types,hyperparams,cates_ind,log=\"training.txt\", n_iter = FULL_TRAINING_STEPS)\n",
    "        #params = train_combinations(X_train_,X_valid_,y_train_,y_valid_,\"neg_mean_absolute_error\",foo_predict_regression,\n",
    "        #                  foo_model_regression,basic_params,d_types,hyperparams,cates_ind,log=\"training.txt\")\n",
    "\n",
    "        \n",
    "        bqs.print_gcps(\"Ready Randomsearch\",bq_log_path=bq_log_path)\n",
    "        best_model =  simple_fit(params,X_train_full,y_train_full,X_valid_,y_valid_,foo_model_regression,basic_params,d_types,cates_ind)\n",
    "        bqs.print_gcps(\"Ready best model\",bq_log_path=bq_log_path)\n",
    "        error = foo_evaluation_regression(y_test,foo_predict_regression(best_model,X_test))\n",
    "        bqs.print_gcps(\"Error:\",error,bq_log_path=bq_log_path)\n",
    "    \n",
    "    c = n_cluster\n",
    "    imp = picture(np.r_[X_train_,X_valid_],np.r_[y_train_,y_valid_],X_test,y_test,cates_ind,best_model,foo_model_regression,train_cols_nocat,cates)\n",
    "    bqs.upload_file(\"lgbmImportance.png\", gcps_path_out + \"lgbmImportance_\"+str(c)+\".png\")\n",
    "    \n",
    "    if select_variables:\n",
    "        imp.sort_values('Value', ascending = False).to_csv(\"importance_short_\"+str(c)+\".csv\",index=False)\n",
    "    else:\n",
    "        imp.sort_values('Value', ascending = False).to_csv(\"importance_full_\"+str(c)+\".csv\",index=False)\n",
    "    \n",
    "    if not select_variables:\n",
    "        bqs.upload_file(\"importance_full_\"+str(c)+\".csv\", gcps_path_out + \"importance_\"+str(c)+\".csv\")\n",
    "    \n",
    "    if select_variables:\n",
    "        return best_model, 0\n",
    "    else:\n",
    "        return best_model, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino columnas de entrenamiento para cada cluster, excluyendo las variables target (data leakage)\n",
    "# y las variables que tuvieron un data drift detectado antes\n",
    "\n",
    "train_cols_by_segment = {}\n",
    "\n",
    "for c in range(N_CLUSTERS):\n",
    "    train_cols_by_segment[c] = list(set(list(summary_cal.columns)) - \\\n",
    "        #set(['CUS_CUST_ID', TARGET_COL,'GMV_TARGET']) - \\\n",
    "        set(EXCLUDE_VARIABLES_DRIFTED[c])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo reducido para la busqueda de variables importantes\n",
    "for c in range(N_CLUSTERS):\n",
    "        bqs.print_gcps(\"-------- ENTRENAMIENTO {} -------\".format(c),bq_log_path=bq_log_path)\n",
    "\n",
    "        #bqs.download_file(gcps_path_in + f'summary_train_{str(c)}.pkl', f'summary_train_{str(c)}.pkl')\n",
    "        #summary_cal = pd.read_pickle(f'summary_train_{str(c)}.pkl')\n",
    "        summary_cluster = summary_cal.loc[summary_cal.cluster == c]\n",
    "        #Corro entrenamiento corto para el cluster seleccionado\n",
    "        bqs.print_gcps(f\"-------- VARIABLES EXCLUIDAS CLUSTER {c} : {model_drifts[c].getDriftedTotal()} -------\", bq_log_path = bq_log_path)\n",
    "\n",
    "        #model, error = modelo_by_segmento(summary_cluster,[x for x in summary_cluster.columns if x not in EXCLUDE_VARIABLES_DRIFT[x]], c, select_variables = True) #modelo para ese segmento\n",
    "        model, error = modelo_by_segmento(summary_cluster,train_cols_by_segment[c], c, select_variables = True) #modelo para ese segmento\n",
    "        with open(\"model_short_\"+str(c)+\".pkl\", 'wb') as file: # guardo\n",
    "            pickle.dump(model, file)\n",
    "\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busqueda de variables importancia en relacion a la variable 'random'\n",
    "for n in range(N_CLUSTERS):\n",
    "    imp = pd.read_csv(f'importance_short_{n}.csv').sort_values('Value', ascending = False)\n",
    "\n",
    "    max_importance = imp.iloc[1,1]\n",
    "    rand_importance = imp.loc[imp.Feature == 'rand'].values[0][1]\n",
    "\n",
    "    print(f\"{n}_max_importance: {max_importance}\")\n",
    "    print(f\"{n}_rand_importance: {rand_importance}\")\n",
    "\n",
    "    mayores_a_rand = list(imp.loc[imp.Value > rand_importance].Feature)\n",
    "    mayores_orden_4 = list(imp.loc[imp.Value >= (max_importance / 1000)].Feature)\n",
    "    distinto_0 = list(imp.loc[imp.Value > 0.0].Feature)\n",
    "\n",
    "    total_vars = [x for x in list(imp.Feature) if (x in mayores_a_rand) and (x in mayores_orden_4) and (x in distinto_0)]\n",
    "\n",
    "    print(f\" Variables Cluster {n}: {len(total_vars)}\")\n",
    "    pd.DataFrame(total_vars, columns = ['Feature']).to_csv(f\"features_model_{n}.csv\", index = False)\n",
    "\n",
    "    # Subo features del modelo N al Storage, incluidos el cluster y la columna target\n",
    "    bqs.upload_file(f\"features_model_{n}.csv\", gcps_path_out + f\"features_model_{n}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo Full productivo\n",
    "errores = []\n",
    "for c in range(N_CLUSTERS):\n",
    "    train_cols = train_cols_by_segment[c]\n",
    "    bqs.print_gcps(\"-------- ENTRENAMIENTO {} -------\".format(c), bq_log_path = bq_log_path)\n",
    "\n",
    "    #bqs.download_file(gcps_path_in + \"summary_train_\"+str(c)+\".pkl\", \"summary_train_\"+str(c)+\".pkl\")\n",
    "    #summary_cal = pd.read_pickle(\"summary_train_\"+str(c)+\".pkl\")\n",
    "\n",
    "    #Variables importantes procesadas en el paso previo + Target\n",
    "    variables = list(pd.read_csv(f\"features_model_{c}.csv\").Feature) + [TARGET_COL]\n",
    "    summary_cluster = summary_cal.loc[summary_cal.cluster == c]\n",
    "    #summary_cal,cates,no_cates = preparo_datos(summary_cal)\n",
    "    # Modelo para ese segmento (Cluster)\n",
    "    model,error = modelo_by_segmento(summary_cluster,variables, c, select_variables = False)\n",
    "    errores.append(error)\n",
    "    bqs.print_gcps(\"-------- ERROR TRAINING {} -------\".format(error), bq_log_path = bq_log_path)\n",
    "    with open(\"model_\"+str(c)+\".pkl\", 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "    bqs.upload_file(\"model_\"+str(c)+\".pkl\", gcps_path_out + \"model_\"+str(c)+\".pkl\")\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from melitk.fda import workspace\n",
    "# metrics_dict = {\n",
    "#             'Error C0': errores[0],\n",
    "#             'Error C1': errores[1],\n",
    "#             'Error C2': errores[2],\n",
    "#             'Error C3': errores[3]\n",
    "#         }\n",
    "    \n",
    "# workspace.save_metrics(metrics_dict)\n",
    "\n",
    "# import pickle\n",
    "# df = pd.DataFrame([1,2,4,5],columns = [\"hola\"])\n",
    "\n",
    "# serialized_dataset = pickle.dumps(df)\n",
    "# workspace.save_raw_model(serialized_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
