{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL - Guideline\n",
    "\n",
    "This notebook was designed with the purpose of guiding you with the development of your first ETL\n",
    "\n",
    "There are many functions and methods that we provide you in the melitk.analytics library (which comes pre-installed), but will not appear in this demo. The general idea behind this demo is for you to understand how the Fury Data Apps expects your ETL process in order to run successfully and generate the dataset you will want to use in your training\n",
    "\n",
    "### Example\n",
    "\n",
    "Here we will generate a dataset that we will use in order to get the probabilities of some users to use the MELI or MP APP in order to make a Cellphone Recharge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "\n",
    "from melitk.analytics.connectors.core.authentication import Authentication\n",
    "from melitk.analytics.connectors.teradata import ConnTeradata\n",
    "from melitk.analytics.connectors.presto import ConnPresto\n",
    "from melitk.fda import workspace\n",
    "\n",
    "from shared.settings import DATASET_FILENAME, SAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teradata_user = os.environ['SECRET_TERADATA_USER']\n",
    "teradata_pass = os.environ['SECRET_TERADATA_PASS']\n",
    "\n",
    "melilake_user = os.environ['SECRET_MELILAKE_USER']\n",
    "melilake_pass = os.environ['SECRET_MELILAKE_PASS']\n",
    "\n",
    "s3_access_key = os.environ['SECRET_S3_ACCESS_KEY']\n",
    "s3_secret_key = os.environ['SECRET_S3_SECRET_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data From Teradata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera = ConnTeradata(teradata_user, teradata_pass, auth_method=Authentication.APP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining queries\n",
    "\n",
    "* Monthly active users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_maus_table = \"\"\"\n",
    "CREATE MULTISET VOLATILE TABLE MAUS, NO LOG AS\n",
    "(\n",
    "SELECT      CUS_CUST_ID,\n",
    "\n",
    "            MAX(CASE \n",
    "            WHEN PHOTO_MES = '201812' AND ZEROIFNULL(DAYS_MAU_MP) > 0 THEN 1\n",
    "            ELSE 0\n",
    "            END )AS MAU_MP_M3,\n",
    "            \n",
    "            MAX(CASE \n",
    "            WHEN PHOTO_MES = '201901' AND ZEROIFNULL(DAYS_MAU_MP) > 0 THEN 1\n",
    "            ELSE 0\n",
    "            END) AS MAU_MP_M2,\n",
    "            \n",
    "            MAX(CASE \n",
    "            WHEN PHOTO_MES = '201902' AND ZEROIFNULL(DAYS_MAU_MP) > 0 THEN 1\n",
    "            ELSE 0\n",
    "            END) AS MAU_MP_M1,\n",
    "            \n",
    "            MAX(CASE \n",
    "            WHEN PHOTO_MES = '201812' AND ZEROIFNULL(DAYS_MAU_ML) > 0 THEN 1\n",
    "            ELSE 0\n",
    "            END) AS MAU_ML_M3,\n",
    "            MAX(CASE \n",
    "            WHEN PHOTO_MES = '201901' AND ZEROIFNULL(DAYS_MAU_ML) > 0 THEN 1\n",
    "            ELSE 0\n",
    "            END) AS MAU_ML_M2,\n",
    "            MAX(CASE \n",
    "            WHEN PHOTO_MES = '201902' AND ZEROIFNULL(DAYS_MAU_ML) > 0 THEN 1\n",
    "            ELSE 0\n",
    "            END) AS MAU_ML_M1,\n",
    "            SUM(CASE \n",
    "            WHEN PHOTO_MES = '201812' AND ZEROIFNULL(DAYS_MAU_ML) + ZEROIFNULL(DAYS_MAU_MP) > 0 THEN 1\n",
    "            ELSE 0\n",
    "            END) AS MAU_M3,\n",
    "            SUM(CASE \n",
    "            WHEN PHOTO_MES = '201901' AND ZEROIFNULL(DAYS_MAU_ML) + ZEROIFNULL(DAYS_MAU_MP) > 0 THEN 1\n",
    "            ELSE 0\n",
    "            END) AS MAU_M2,\n",
    "            SUM(CASE\n",
    "            WHEN PHOTO_MES = '201902' AND ZEROIFNULL(DAYS_MAU_ML) + ZEROIFNULL(DAYS_MAU_MP) > 0 THEN 1\n",
    "            ELSE 0\n",
    "            END) AS MAU_M1,\n",
    "            SUM(CASE\n",
    "            WHEN PHOTO_MES = '201812' THEN ZEROIFNULL(DAYS_MAU_ML) + ZEROIFNULL(DAYS_MAU_MP)\n",
    "            ELSE 0\n",
    "            END) AS DAYS_MAU_M3,\n",
    "            SUM(CASE\n",
    "            WHEN PHOTO_MES = '201901' THEN ZEROIFNULL(DAYS_MAU_ML) + ZEROIFNULL(DAYS_MAU_MP)\n",
    "            ELSE 0\n",
    "            END) AS DAYS_MAU_M2,\n",
    "            SUM(CASE\n",
    "            WHEN PHOTO_MES = '201902' THEN ZEROIFNULL(DAYS_MAU_ML) + ZEROIFNULL(DAYS_MAU_MP)\n",
    "            ELSE 0\n",
    "            END) AS DAYS_MAU_M1\n",
    "            \n",
    "FROM        WHOWNER.LK_WALLET_MAUS\n",
    "\n",
    "WHERE       SIT_SITE_ID = 'MLA'\n",
    "            AND PHOTO_MES BETWEEN '201812' AND '201902'\n",
    "            \n",
    "GROUP BY 1\n",
    ") WITH DATA UNIQUE PRIMARY INDEX (CUS_CUST_ID) ON COMMIT PRESERVE ROWS\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing query and getting result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tera.execute(create_maus_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define this sample var during dev to speed-up the etl (results won't be good).\n",
    "maus_result = tera.execute_response(\"SELECT * FROM MAUS SAMPLE {}\".format(SAMPLE))\n",
    "maus_df = pd.DataFrame(maus_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data From Melilake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presto = ConnPresto(melilake_user, melilake_pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mp_payments.sql', 'r') as query_file:\n",
    "    mp_payments = query_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing query and getting result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_payments_result = presto.execute_response(mp_payments)\n",
    "mp_payments_df = pd.DataFrame(mp_payments_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data From AWS S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(key=s3_access_key, secret=s3_secret_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_payment_methods_df = pd.read_csv( s3.open(path=\"s3://bi-public-data/training/lk_mp_pay_payment_methods.csv\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_payers_merge_df = pd.merge(mp_payments_df, \n",
    "                            mp_payment_methods_df, \n",
    "                            how='inner',  \n",
    "                            left_on='PAY_PM_ID', \n",
    "                            right_on='PAY_PM_TYPE_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_payers_merge_df.columns = map(str.upper, wallet_payers_merge_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_payers_merge_df = wallet_payers_merge_df[[\n",
    "    'CUS_CUST_ID',\n",
    "    'PAY_MOVE_DATE',\n",
    "    'TPV_SEGMENT_DETAIL',\n",
    "    'PAY_PAYMENT_ID',\n",
    "    'PAY_COUPON_AMOUNT_AMT',\n",
    "    'PAY_PM_TYPE_DESC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_payers_merge_df['YEAR_MONTH_DATE'] = pd.to_datetime(wallet_payers_merge_df['PAY_MOVE_DATE']).dt.strftime('%Y%m').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wallet_payers_merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pagos Mensuales\n",
    "payments_m3 = (df[ df.YEAR_MONTH_DATE == 201812 ]\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['PAY_MOVE_DATE']]\n",
    "    .rename(columns={'PAY_MOVE_DATE':'PAYMENTS_M3'}))\n",
    "\n",
    "payments_m2 = (df[ df.YEAR_MONTH_DATE == 201901 ]\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['PAY_MOVE_DATE']]\n",
    "    .rename(columns={'PAY_MOVE_DATE':'PAYMENTS_M2'}))\n",
    "\n",
    "payments_m1 = (df[ df.YEAR_MONTH_DATE == 201902 ]\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['PAY_MOVE_DATE']]\n",
    "    .rename(columns={'PAY_MOVE_DATE':'PAYMENTS_M1'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cantidad de dias que opero\n",
    "days_w_payments_m3 = (df[ df.YEAR_MONTH_DATE == 201812 ]\n",
    "    .groupby(['CUS_CUST_ID','PAY_MOVE_DATE'])\n",
    "    .count()\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['TPV_SEGMENT_DETAIL']]\n",
    "    .rename(columns={'TPV_SEGMENT_DETAIL':'DAYS_W_PAYMENTS_M3'}))\n",
    "   \n",
    "days_w_payments_m2 = (df[ df.YEAR_MONTH_DATE == 201901 ]\n",
    "    .groupby(['CUS_CUST_ID','PAY_MOVE_DATE'])\n",
    "    .count()\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['TPV_SEGMENT_DETAIL']]\n",
    "    .rename(columns={'TPV_SEGMENT_DETAIL':'DAYS_W_PAYMENTS_M2'}))\n",
    "\n",
    "days_w_payments_m1 = (df[ df.YEAR_MONTH_DATE == 201902 ]\n",
    "    .groupby(['CUS_CUST_ID','PAY_MOVE_DATE'])\n",
    "    .count()\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['TPV_SEGMENT_DETAIL']]\n",
    "    .rename(columns={'TPV_SEGMENT_DETAIL':'DAYS_W_PAYMENTS_M1'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pagos Mensuales de Cellphone Rech.\n",
    "payments_cp_r_m3 = (df[ (df.YEAR_MONTH_DATE == 201812) & (df.TPV_SEGMENT_DETAIL == 'Cellphone Recharge') ]\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['PAY_MOVE_DATE']]\n",
    "    .rename(columns={'PAY_MOVE_DATE':'PAYMENTS_CP_R_M3'}))\n",
    "\n",
    "payments_cp_r_m2 = (df[ (df.YEAR_MONTH_DATE == 201901) & (df.TPV_SEGMENT_DETAIL == 'Cellphone Recharge') ]\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['PAY_MOVE_DATE']]\n",
    "    .rename(columns={'PAY_MOVE_DATE':'PAYMENTS_CP_R_M2'}))\n",
    "\n",
    "payments_cp_r_m1 = (df[ (df.YEAR_MONTH_DATE == 201902) & (df.TPV_SEGMENT_DETAIL == 'Cellphone Recharge') ]\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['PAY_MOVE_DATE']]\n",
    "    .rename(columns={'PAY_MOVE_DATE':'PAYMENTS_CP_R_M1'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pagos Mensuales sin descuento\n",
    "payments_no_disc_m3 = (df[ (df.YEAR_MONTH_DATE == 201812) & (df.PAY_COUPON_AMOUNT_AMT == 0) ]\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['PAY_MOVE_DATE']]\n",
    "    .rename(columns={'PAY_MOVE_DATE':'PAYMENTS_NO_DISC_M3'}))\n",
    "\n",
    "payments_no_disc_m2 = (df[ (df.YEAR_MONTH_DATE == 201901) & (df.PAY_COUPON_AMOUNT_AMT == 0) ]\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['PAY_MOVE_DATE']]\n",
    "    .rename(columns={'PAY_MOVE_DATE':'PAYMENTS_NO_DISC_M2'}))\n",
    "\n",
    "payments_no_disc_m1 = (df[ (df.YEAR_MONTH_DATE == 201902) & (df.PAY_COUPON_AMOUNT_AMT == 0) ]\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['PAY_MOVE_DATE']]\n",
    "    .rename(columns={'PAY_MOVE_DATE':'PAYMENTS_NO_DISC_M1'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pagos Mensuales sin descuento de Cellphone Rech.\n",
    "payments_no_disc_cp_r_m3 = (df[ (df.YEAR_MONTH_DATE == 201812) & (df.PAY_COUPON_AMOUNT_AMT == 0) & (df.TPV_SEGMENT_DETAIL == 'Cellphone Recharge') ]\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['PAY_MOVE_DATE']]\n",
    "    .rename(columns={'PAY_MOVE_DATE':'PAYMENTS_NO_DISC_CP_R_M3'}))\n",
    "\n",
    "payments_no_disc_cp_r_m2 = (df[ (df.YEAR_MONTH_DATE == 201901) & (df.PAY_COUPON_AMOUNT_AMT == 0) & (df.TPV_SEGMENT_DETAIL == 'Cellphone Recharge') ]\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['PAY_MOVE_DATE']]\n",
    "    .rename(columns={'PAY_MOVE_DATE':'PAYMENTS_NO_DISC_CP_R_M2'}))\n",
    "\n",
    "payments_no_disc_cp_r_m1 = (df[ (df.YEAR_MONTH_DATE == 201902) & (df.PAY_COUPON_AMOUNT_AMT == 0) & (df.TPV_SEGMENT_DETAIL == 'Cellphone Recharge') ]\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['PAY_MOVE_DATE']]\n",
    "    .rename(columns={'PAY_MOVE_DATE':'PAYMENTS_NO_DISC_CP_R_M1'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primer y ultimo pago\n",
    "first_payment_m3 = ( df[ df.PAY_MOVE_DATE <= '2019-02-28' ]\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .agg({'PAY_MOVE_DATE':'min'})\n",
    "    .rename(columns={'PAY_MOVE_DATE':'FIRST_PAYMENT_M3'}))\n",
    "\n",
    "last_payment_m3 = ( df[ df.PAY_MOVE_DATE <= '2019-02-28' ]\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .agg({'PAY_MOVE_DATE':'max'})\n",
    "    .rename(columns={'PAY_MOVE_DATE':'LAST_PAYMENT_M3'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pagos Por Method\n",
    "payments_acc_money = (df[ df.PAY_PM_TYPE_DESC == 'Account Money' ]\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['PAY_MOVE_DATE']]\n",
    "    .rename(columns={'PAY_MOVE_DATE':'PAYMENTS_ACC_MONEY'}))\n",
    "\n",
    "payments_credit_card = (df[ df.PAY_PM_TYPE_DESC == 'Credit Card' ]\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['PAY_MOVE_DATE']]\n",
    "    .rename(columns={'PAY_MOVE_DATE':'PAYMENTS_CREDIT_CARD'}))\n",
    "\n",
    "payments_debit_card = (df[ df.PAY_PM_TYPE_DESC == 'Debit Card' ]\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .count()[['PAY_MOVE_DATE']]\n",
    "    .rename(columns={'PAY_MOVE_DATE':'PAYMENTS_DEBIT_CARD'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target\n",
    "df['TARGET'] = 0\n",
    "df.loc[ (df.YEAR_MONTH_DATE == 201903) & (df.TPV_SEGMENT_DETAIL == 'Cellphone Recharge'), 'TARGET'] = 1\n",
    "\n",
    "target = (\n",
    "    df\n",
    "    .groupby('CUS_CUST_ID')\n",
    "    .agg({'TARGET':'max'})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join all New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LK Customers\n",
    "customers = (\n",
    "    df\n",
    "    .groupby('CUS_CUST_ID')[['PAY_MOVE_DATE']]\n",
    "    .count()).drop('PAY_MOVE_DATE',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.merge(customers,payments_m3,how='left',on='CUS_CUST_ID')\n",
    "features = pd.merge(features,payments_m2,how='left',on='CUS_CUST_ID')\n",
    "features = pd.merge(features,payments_m1,how='left',on='CUS_CUST_ID')\n",
    "\n",
    "features = pd.merge(features,days_w_payments_m3,how='left',on='CUS_CUST_ID')\n",
    "features = pd.merge(features,days_w_payments_m2,how='left',on='CUS_CUST_ID')\n",
    "features = pd.merge(features,days_w_payments_m1,how='left',on='CUS_CUST_ID')\n",
    "\n",
    "features = pd.merge(features,payments_cp_r_m3,how='left',on='CUS_CUST_ID')\n",
    "features = pd.merge(features,payments_cp_r_m2,how='left',on='CUS_CUST_ID')\n",
    "features = pd.merge(features,payments_cp_r_m1,how='left',on='CUS_CUST_ID')\n",
    "\n",
    "features = pd.merge(features,payments_no_disc_m3,how='left',on='CUS_CUST_ID')\n",
    "features = pd.merge(features,payments_no_disc_m2,how='left',on='CUS_CUST_ID')\n",
    "features = pd.merge(features,payments_no_disc_m1,how='left',on='CUS_CUST_ID')\n",
    "\n",
    "features = pd.merge(features,payments_no_disc_cp_r_m3,how='left',on='CUS_CUST_ID')\n",
    "features = pd.merge(features,payments_no_disc_cp_r_m2,how='left',on='CUS_CUST_ID')\n",
    "features = pd.merge(features,payments_no_disc_cp_r_m1,how='left',on='CUS_CUST_ID')\n",
    "\n",
    "features = pd.merge(features,first_payment_m3,how='left',on='CUS_CUST_ID')\n",
    "features = pd.merge(features,last_payment_m3,how='left',on='CUS_CUST_ID')\n",
    "\n",
    "features = pd.merge(features,payments_acc_money,how='left',on='CUS_CUST_ID')\n",
    "features = pd.merge(features,payments_credit_card,how='left',on='CUS_CUST_ID')\n",
    "features = pd.merge(features,payments_debit_card,how='left',on='CUS_CUST_ID')\n",
    "\n",
    "features = pd.merge(features,target,how='left',on='CUS_CUST_ID')\n",
    "\n",
    "features = features.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_payers_df = features.reset_index().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking format and handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns_maus_df = [\n",
    "    'MAU_MP_M3', 'MAU_MP_M2', 'MAU_MP_M1', 'MAU_ML_M3', 'MAU_ML_M2', 'MAU_ML_M1', \n",
    "    'MAU_M3', 'MAU_M2', 'MAU_M1', 'DAYS_MAU_M3', 'DAYS_MAU_M2','DAYS_MAU_M1'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns_wallet_payers_df = [\n",
    "    'PAYMENTS_M3', 'PAYMENTS_M2', 'PAYMENTS_M1',\n",
    "    'DAYS_W_PAYMENTS_M3', 'DAYS_W_PAYMENTS_M2', 'DAYS_W_PAYMENTS_M1',\n",
    "    'PAYMENTS_CP_R_M3', 'PAYMENTS_CP_R_M2', 'PAYMENTS_CP_R_M1',\n",
    "    'PAYMENTS_NO_DISC_M3', 'PAYMENTS_NO_DISC_M2', 'PAYMENTS_NO_DISC_M1',\n",
    "    'PAYMENTS_NO_DISC_CP_R_M3', 'PAYMENTS_NO_DISC_CP_R_M2', 'PAYMENTS_NO_DISC_CP_R_M1',\n",
    "    'PAYMENTS_ACC_MONEY', 'PAYMENTS_CREDIT_CARD', 'PAYMENTS_DEBIT_CARD'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maus_df[numeric_columns_maus_df] = maus_df[numeric_columns_maus_df].fillna(0)\n",
    "wallet_payers_df[numeric_columns_wallet_payers_df] = wallet_payers_df[numeric_columns_wallet_payers_df].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_payers_df['FIRST_PAYMENT_M3'] = pd.to_datetime(wallet_payers_df['FIRST_PAYMENT_M3'])\n",
    "wallet_payers_df['LAST_PAYMENT_M3'] = pd.to_datetime(wallet_payers_df['LAST_PAYMENT_M3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_date = datetime.datetime(2019, 2, 28, 0, 0, 0, 0)\n",
    "wallet_payers_df['DATE_BEFORE_TARGET'] = target_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_payers_df['DAYS_SINCE_LAST_PAYMENT'] = (\n",
    "    wallet_payers_df['DATE_BEFORE_TARGET'] - wallet_payers_df['LAST_PAYMENT_M3']\n",
    ").dt.days\n",
    "\n",
    "wallet_payers_df['DAYS_SINCE_FIRST_PAYMENT'] = (\n",
    "    wallet_payers_df['DATE_BEFORE_TARGET'] - wallet_payers_df['FIRST_PAYMENT_M3']\n",
    ").dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_date_cols = ['DAYS_SINCE_LAST_PAYMENT', 'DAYS_SINCE_FIRST_PAYMENT']\n",
    "wallet_payers_df[relative_date_cols] = wallet_payers_df[relative_date_cols].fillna(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallet_payers_df[['CUS_CUST_ID', 'TARGET']].groupby('TARGET').agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(wallet_payers_df, maus_df, on='CUS_CUST_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['CUS_CUST_ID', 'FIRST_PAYMENT_M3', 'LAST_PAYMENT_M3', 'DATE_BEFORE_TARGET']\n",
    "df = df.drop(columns_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our job\n",
    "By saving the dataset this way, we are letting the platform handle the storage of our data, which guarantees us that\n",
    "all the steps will be able to access to the appropriate paths without having to create any storage nor \n",
    "asking for roles, credentials, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train and test, and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving 'TARGET' column to the last place just to be able to assemble train and dev datasets easily\n",
    "cols_at_end = ['TARGET']\n",
    "df = df[[c for c in df if c not in cols_at_end] + [c for c in cols_at_end if c in df]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asserting that I am only working with 2 classes, as I was expecting for this demo\n",
    "df['TARGET'] = df['TARGET'].fillna(value=0)\n",
    "assert(len(df['TARGET'].unique()) in [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting Pandas dataframe is serialized using Python's standard pickle module\n",
    "serialized_dataset = pickle.dumps(df)\n",
    "workspace.save_etl_file(DATASET_FILENAME, serialized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for validation purposes\n",
    "loaded_obj = workspace.load_etl_file(DATASET_FILENAME)\n",
    "loaded_dataset = pickle.loads(loaded_obj)\n",
    "assert(len(df) == len(loaded_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
