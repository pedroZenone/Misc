{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "\n",
    "from melitk.analytics.connectors.core.authentication import Authentication\n",
    "from melitk.analytics.connectors.teradata import ConnTeradata\n",
    "from melitk.analytics.connectors.presto import ConnPresto\n",
    "from melitk.fda import workspace\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from shared.settings import DATASET_FILENAME, SAMPLE\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from lifetimes.utils import summary_data_from_transaction_data\n",
    "import numpy as np\n",
    "import datetime\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "from lifetimes import ModifiedBetaGeoFitter\n",
    "from lifetimes import GammaGammaFitter\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "import joblib\n",
    "import boto3\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "cores = cpu_count() - 1\n",
    "\n",
    "s3_path = \"ltv-ml/ML/MLM/LTV_long/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _write_dataframe_to_csv_on_s3(df, path_s3):\n",
    "    import boto3\n",
    "    from io import StringIO\n",
    "    \"\"\" Write a dataframe to a CSV on S3 \"\"\"\n",
    "    a = path_s3.split('//')\n",
    "    b = a[1].split('/')\n",
    "    bucket = b[0]\n",
    "    c = path_s3.split(bucket+'/')\n",
    "    path = c[1]\n",
    "    \n",
    "    buffer = StringIO()\n",
    "    df.to_csv(buffer,index=False)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Object(bucket, path).put(Body=buffer.getvalue())\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levanto la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asigno_quintiles_4(x):\n",
    "    step1 = int(x.shape[0]*0.1) \n",
    "    step2 = int(x.shape[0]*0.2)\n",
    "    \n",
    "    l_ = []\n",
    "    l_ = l_ + [\"q\"+str(1)]*step1 \n",
    "    l_ = l_ + [\"q\"+str(2)]*step1 \n",
    "    l_ = l_ + [\"q\"+str(3)]*step2 \n",
    "    \n",
    "    step3 = x.shape[0] - len(l_)\n",
    "    l_ = l_ + [\"q4\"]*step3\n",
    "    return l_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aMultiprocesar_IPT(x):\n",
    "    aux = x.sort_values(by=[\"cust\",\"date\"])\n",
    "    aux = aux.drop_duplicates(subset=[\"cust\",\"date\"]) # para que el IPT no de infinito \n",
    "    \n",
    "    if(aux.shape[0] == 0):\n",
    "        return pd.DataFrame([],columns = [\"cust\",\"IPT_mean\",\"IPT_std\",\"IPT_max\",\"IPT_min\",\"IPT_sum\",\"IPT_CV\"])\n",
    "    aux = aux.groupby(\"cust\").apply(lambda x: x[\"date\"].diff()).reset_index()\n",
    "\n",
    "    if(\"date\" not in aux.columns):\n",
    "        return pd.DataFrame([],columns = [\"cust\",\"IPT_mean\",\"IPT_std\",\"IPT_max\",\"IPT_min\",\"IPT_sum\",\"IPT_CV\"])\n",
    "    \n",
    "    aux = aux[[\"cust\",\"date\"]] \n",
    "    aux= aux.dropna(subset = [\"date\"]) ## saco los nan para que no rompa el log10\n",
    "    if(aux.shape[0] == 0):\n",
    "        return pd.DataFrame([],columns = [\"cust\",\"IPT_mean\",\"IPT_std\",\"IPT_max\",\"IPT_min\",\"IPT_sum\",\"IPT_CV\"])\n",
    "    aux[\"date\"] = aux[\"date\"].apply(lambda x: np.log10(x/ np.timedelta64(1, 'D')))\n",
    "    if(aux.shape[0] == 0):\n",
    "        return pd.DataFrame([],columns = [\"cust\",\"IPT_mean\",\"IPT_std\",\"IPT_max\",\"IPT_min\",\"IPT_sum\",\"IPT_CV\"])\n",
    "    \n",
    "    aux =  aux.groupby(\"cust\")[\"date\"].agg({\"IPT_mean\":\"mean\",\"IPT_std\":\"std\",\"IPT_max\":\"max\",\"IPT_min\":\"min\",\"IPT_sum\":\"sum\"}).reset_index()\n",
    "    aux[\"IPT_CV\"] = aux[\"IPT_std\"]/aux[\"IPT_mean\"]\n",
    "    return aux[[\"cust\",\"IPT_mean\",\"IPT_std\",\"IPT_max\",\"IPT_min\",\"IPT_sum\",\"IPT_CV\"]] \n",
    "\n",
    "def aMultiprocesar_sales(x):\n",
    "    aux =  x.groupby(\"cust\")[\"sales\"].agg({\"money_mean\":\"mean\",\"money_std\":\"std\",\"money_max\":\"max\",\"money_min\":\"min\",\n",
    "                                        \"money_sum\":\"sum\"}).reset_index()\n",
    "    aux[\"money_CV\"] = aux[\"money_std\"]/aux[\"money_mean\"]\n",
    "    return aux[[\"cust\",\"money_sum\",\"money_CV\",\"money_mean\",\"money_std\",\"money_max\",\"money_min\"]]\n",
    "    \n",
    "def preprocesing(country_cal):\n",
    "    \n",
    "    ########################### Preproc para pareto  ###########################\n",
    "    print(\"Preproc para pareto\")\n",
    "    \n",
    "    summary_cal = summary_data_from_transaction_data(country_cal, 'cust', 'date', freq=\"W\",\n",
    "                                                     monetary_value_col='sales',  # le saca la primer compra (semana de compras) y toma el promedio agrupado por semana (frequency)\n",
    "                                            observation_period_end=country_cal.date.max()) \n",
    "    \n",
    "    ########################### My recency   ###########################\n",
    "    print(\"Recency frequency\")\n",
    "    df = country_cal.groupby('cust')[\"date\"].agg({\"date_max\":\"max\",\"date_min\":\"min\"}).reset_index()\n",
    "    df[\"my_recency\"] = (country_cal.date.max()  - df[\"date_max\"])/np.timedelta64(1, \"W\") # me genero el recency y la semanalizo\n",
    "    df[[\"my_recency\"]] = df[[\"my_recency\"]].applymap(np.int64)\n",
    "\n",
    "    summary_cal = pd.merge(summary_cal,df,on=\"cust\",how=\"left\").fillna(0)\n",
    "    \n",
    "    ########################### Pareto  ###########################\n",
    "    bgf = ModifiedBetaGeoFitter()\n",
    "    bgf.fit(summary_cal['frequency'], summary_cal['recency'], summary_cal['T'])\n",
    "    print(\"fiteo\")\n",
    "    summary_cal[\"n_purchases_pred\"] = dd.from_pandas(summary_cal, npartitions=cores).apply(lambda x: bgf.conditional_expected_number_of_purchases_up_to_time(52*7, x['frequency'], x['recency'], x['T']),axis = 1,meta = ('float')).compute(scheduler='processes')\n",
    "    print(\"npred\")\n",
    "\n",
    "    def apply_pAlive(x):  # hay modelos que devuelve una lista como proba, hay que hacer un unpak en estos casos\n",
    "        y = bgf.conditional_probability_alive(x['frequency'], x['recency'], x['T'])\n",
    "        if(type(y) == np.ndarray):\n",
    "            return y[0]\n",
    "        else:\n",
    "            return y\n",
    "\n",
    "    summary_cal[\"p_alive\"] = dd.from_pandas(summary_cal, npartitions=cores).apply(lambda x: apply_pAlive(x),axis = 1,meta = ('float')).compute(scheduler='processes')\n",
    "    print(\"p_alive\")\n",
    "\n",
    "    # Si imputo valores crashea ya que no puedo converger, intente moviendole el penalizer pero la cosa no mejora...\n",
    "    summary_gg = summary_cal.copy()\n",
    "    summary_gg.loc[summary_gg[\"monetary_value\"] == 0,\"monetary_value\"] = min(summary_gg.loc[summary_gg[\"monetary_value\"] > 0][\"monetary_value\"])\n",
    "    #summary_cal = summary_cal.loc[summary_cal[\"monetary_value\"] > 0]\n",
    "    ggf = GammaGammaFitter(penalizer_coef = 0)\n",
    "    ggf.fit(summary_gg['frequency']+1,\n",
    "            summary_gg['monetary_value'])\n",
    "\n",
    "    print(\"fit\")\n",
    "\n",
    "    resu = ggf.conditional_expected_average_profit(summary_gg['frequency'],\n",
    "            summary_gg['monetary_value']).reset_index()\n",
    "\n",
    "    resu.columns = [\"ind\",\"montary_sales_forecast\"]\n",
    "    summary_cal[\"montary_sales_forecast\"] = resu[\"montary_sales_forecast\"]\n",
    "    # la cantidad de compras que le va a hacer el usuario en su año de vida es la cantidad de compras que hara * compras promedio\n",
    "    summary_cal[\"clv_pareto\"] = summary_cal[\"n_purchases_pred\"]*summary_cal[\"montary_sales_forecast\"] \n",
    "    summary_cal[\"target_pareto\"] = summary_cal[\"n_purchases_pred\"]*summary_cal[\"clv_pareto\"]    \n",
    "    \n",
    "    ########################### IPT  ###########################\n",
    "    print(\"IPT\")\n",
    "    cust_ids = list(country_cal[\"cust\"].unique())\n",
    "\n",
    "    def chunks(l, n):\n",
    "        \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "        for i in range(0, len(l), n):\n",
    "            yield l[i:i + n]\n",
    "\n",
    "    l_dfs = []\n",
    "    for i in chunks(cust_ids,len(cust_ids)//cores):\n",
    "        l_dfs.append(country_cal.loc[country_cal[\"cust\"].isin(i)])\n",
    "\n",
    "    p = Pool(cpu_count())\n",
    "    l_procedDfs = list(tqdm(p.imap(aMultiprocesar_IPT,l_dfs), total=len(l_dfs)))\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "    df = pd.concat(l_procedDfs)\n",
    "    summary_cal = pd.merge(summary_cal,df,on=\"cust\",how=\"left\").fillna(0)\n",
    "    \n",
    "   \n",
    "    ########################### Sales  ############################\n",
    "    print(\"sales\")\n",
    "    cust_ids = list(country_cal[\"cust\"].unique())\n",
    "\n",
    "    def chunks(l, n):\n",
    "        \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "        for i in range(0, len(l), n):\n",
    "            yield l[i:i + n]\n",
    "\n",
    "    l_dfs = []\n",
    "    for i in chunks(cust_ids,len(cust_ids)//cores):\n",
    "        l_dfs.append(country_cal.loc[country_cal[\"cust\"].isin(i)])\n",
    "\n",
    "      \n",
    "\n",
    "    p = Pool(cpu_count())\n",
    "    l_procedDfs = list(tqdm(p.imap(aMultiprocesar_sales,l_dfs), total=len(l_dfs)))\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "    df = pd.concat(l_procedDfs)\n",
    "    summary_cal = pd.merge(summary_cal,df,on=\"cust\",how=\"left\").fillna(0)\n",
    "    \n",
    "    return summary_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_porcentage(country_cal,vectorizer,L_num):\n",
    "\n",
    "    # saco los na..\n",
    "    country_cal = country_cal.dropna(subset=[L_num])\n",
    "    country_cal[L_num] = country_cal[L_num].apply(lambda x: str(int(x)))\n",
    "\n",
    "    def categories(y):    \n",
    "        dtm = vectorizer.transform(y[L_num].values)  # a sparse matrix\n",
    "        vocab = vectorizer.get_feature_names()  # a list\n",
    "        words_freq = np.asarray(dtm.sum(axis=0))      \n",
    "\n",
    "        return pd.DataFrame(words_freq / words_freq.sum(),columns = vocab)\n",
    "\n",
    "    cust_ids = list(country_cal[\"cust\"].unique())\n",
    "\n",
    "    def chunks(l, n):\n",
    "        \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "        for i in range(0, len(l), n):\n",
    "            yield l[i:i + n]\n",
    "\n",
    "    l_dfs = []\n",
    "    for i in chunks(cust_ids,len(cust_ids)//8):\n",
    "        l_dfs.append(country_cal.loc[country_cal[\"cust\"].isin(i)])\n",
    "\n",
    "    aux = []\n",
    "    for batch in l_dfs:\n",
    "        aux.append(\n",
    "            dd.from_pandas(batch, npartitions=cores).groupby(\"cust\").apply(\n",
    "                lambda x:categories(x)).compute(scheduler='processes').reset_index().drop([\"level_1\"],axis = 1))\n",
    "        os.write(1,b\"\\ntermino batch\")\n",
    "    \n",
    "    L = L_num.split('_')[1]\n",
    "    resu = pd.concat(aux)\n",
    "    resu.columns = [x if(x == \"cust\") else L + \"_\"+x for x in resu.columns]\n",
    "    return resu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quintil_past(country_qpast):\n",
    "    aux_f = country_qpast.set_index(\"date\").to_period(\"W\").to_timestamp().reset_index().drop_duplicates(\n",
    "        subset = [\"date\",\"cust\"]).groupby(\"cust\")[\"sales\"].agg([\"size\"]).reset_index()    \n",
    "    aux_f.columns = [\"cust\",\"freq_q\"]\n",
    "    print(country_qpast.columns)\n",
    "    aux_s = country_qpast.groupby(\"cust\")[\"sales\"].sum().reset_index()\n",
    "    print(aux_s.columns)\n",
    "    aux_s.columns = [\"cust\",\"sales_q\"]     \n",
    "    \n",
    "    aux = pd.merge(aux_f,aux_s,on=\"cust\",how = \"outer\")\n",
    "    aux[\"clv_q\"] = aux[\"freq_q\"]*aux[\"sales_q\"]\n",
    "    \n",
    "    aux = aux.sort_values(by = \"clv_q\",ascending = False)\n",
    "    aux[\"quintil_past\"] = asigno_quintiles_4(aux)\n",
    "    \n",
    "    aux = aux.replace({\"quintil_past\":{\"q4\":0,\"q3\":1,\"q2\":2,\"q1\":3}})\n",
    "    return aux[[\"cust\",\"quintil_past\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_data_size(summary_1,inic,country):\n",
    "    for i in range(4):\n",
    "        c_aux = country.loc[(country.date < inic - relativedelta(months = i)) & (country.date >= inic - relativedelta(months = (i+1)))]\n",
    "        aux = c_aux.drop_duplicates(\n",
    "            subset = [\"cust\",\"date\"]).groupby(\"cust\").size().reset_index(name = \"size\")[[\"cust\",\"size\"]]\n",
    "        aux.columns = [\"cust\",\"size_\"+str(i)]\n",
    "        summary_1 = pd.merge(summary_1,aux,on=\"cust\",how = \"left\").fillna(0)\n",
    "    return summary_1\n",
    "\n",
    "def month_data_sales(summary_1,inic,country):\n",
    "    for i in range(4):\n",
    "        c_aux = country.loc[(country.date < inic - relativedelta(months = (i))) & (country.date >= inic - relativedelta(months = (i+1)))]\n",
    "        aux = c_aux.groupby(\"cust\").sum().reset_index()[[\"cust\",\"sales\"]]\n",
    "        aux.columns = [\"cust\",\"sales_\"+str(i)]\n",
    "        summary_1 = pd.merge(summary_1,aux,on=\"cust\",how = \"left\").fillna(0)\n",
    "    return summary_1\n",
    "\n",
    "def month_data_recency(summary_1,inic,country):\n",
    "    for i in range(4):\n",
    "        c_aux = country.loc[(country.date < inic - relativedelta(months = (i))) & (country.date >= inic - relativedelta(months = (i+1)))]\n",
    "        aux = c_aux.groupby('cust')[\"date\"].agg({\"date_max\":\"max\",\"date_min\":\"min\"}).reset_index()\n",
    "        aux[\"my_recency\"] = (c_aux.date.max()  - aux[\"date_max\"])/np.timedelta64(1, \"W\") # me genero el recency y la semanalizo\n",
    "        aux = aux[[\"cust\",\"my_recency\"]]\n",
    "        aux.columns = [\"cust\",\"recency_\"+str(i)]\n",
    "        summary_1 = pd.merge(summary_1,aux,on=\"cust\",how = \"left\").fillna(-1)\n",
    "    return summary_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_real_date(actual,expect):\n",
    "    resta = (datetime.date(actual.year,actual.month,actual.day) - expect)\n",
    "    if(resta.days < 0):\n",
    "        return datetime.date(actual.year -1,expect.month,expect.day)\n",
    "    return expect\n",
    "\n",
    "# me quedo con el hotsale que este mas cerca\n",
    "def nearest_date(items,pivot):\n",
    "    pivot = datetime.date(pivot.year,pivot.month,pivot.day)\n",
    "    difs = [np.abs((x - pivot).days) for x in items ]\n",
    "    return items[np.argmin(difs)]\n",
    "\n",
    "def last_hot(summary_1,near_hot,country):    \n",
    "    c_aux = country.loc[(country.date < (near_hot + relativedelta(days = 5))) & (country.date >= (near_hot - relativedelta(days = (5))))]\n",
    "    aux = c_aux.drop_duplicates(\n",
    "        subset = [\"cust\",\"date\"]).groupby(\"cust\").size().reset_index(name = \"size\")[[\"cust\",\"size\"]]\n",
    "    aux.columns = [\"cust\",\"size_last_hot\"]\n",
    "    summary_1 = pd.merge(summary_1,aux,on=\"cust\",how = \"left\").fillna(0)\n",
    "    \n",
    "    return summary_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_lag_12(summary_1,inic,country):    \n",
    "    c_aux = country.loc[(country.date < (inic - relativedelta(months = 12-3))) & (country.date >= (inic - relativedelta(months = (12))))]\n",
    "    print(c_aux.date.min(), c_aux.date.max())\n",
    "    aux = c_aux.drop_duplicates(\n",
    "        subset = [\"cust\",\"date\"]).groupby(\"cust\").size().reset_index(name = \"size\")[[\"cust\",\"size\"]]\n",
    "    aux.columns = [\"cust\",\"size_lag_12\"]\n",
    "    summary_1 = pd.merge(summary_1,aux,on=\"cust\",how = \"left\").fillna(0)\n",
    "    return summary_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pd.read_csv(\"s3://fda-labs/ltv-ml/ML/MLM/Datasets/Location.csv\",sep = \"|\")\n",
    "locations[\"SHP_ADD_STATE_NAME_R\"] = locations[\"SHP_ADD_STATE_NAME_R\"].str.lower()\n",
    "\n",
    "locations.loc[locations.SHP_ADD_STATE_NAME_R == \"distrito federal\",\"SHP_ADD_STATE_NAME_R\"] = \"estado de méxico\"\n",
    "locations.loc[locations.SHP_ADD_STATE_NAME_R == \"estado de mexico\",\"SHP_ADD_STATE_NAME_R\"] = \"estado de méxico\"\n",
    "locations.loc[locations.SHP_ADD_STATE_NAME_R == \"ciudad de mexico\",\"SHP_ADD_STATE_NAME_R\"] = \"estado de méxico\"\n",
    "locations.loc[locations.SHP_ADD_STATE_NAME_R == \"nuevo leon\",\"SHP_ADD_STATE_NAME_R\"] = \"nuevo león\"\n",
    "locations.loc[locations.SHP_ADD_STATE_NAME_R == \"san luis potosi\",\"SHP_ADD_STATE_NAME_R\"] = \"san luis potosí\"\n",
    "locations.loc[locations.SHP_ADD_STATE_NAME_R == \"yucatan\",\"SHP_ADD_STATE_NAME_R\"] = \"yucatán\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = pd.read_csv(\"s3://fda-labs/ltv-ml/ML/MLM/Datasets/Raw_Extend_MLM_1_2019-07-20.csv\",sep = \"|\").append(\n",
    "    pd.read_csv(\"s3://fda-labs/ltv-ml/ML/MLM/Datasets/Raw_Extend_MLM_2_2019-07-20.csv\",sep = \"|\"),ignore_index = True)\n",
    "#country = pd.read_csv(\"s3://fda-labs/ltv-ml/ML/sample_Raw_MLA.csv\")[[\"date\",\"cust\",\"sales\",\"CATEG_L1\",\"CATEG_L2\",\"CATEG_L3\"]]\n",
    "country.columns = [\"date\",\"cust\",\"sales\",\"CATEG_L1\",\"CATEG_L2\",\"CATEG_L3\"]\n",
    "country[\"date\"] = pd.to_datetime(country[\"date\"],infer_datetime_format=True,errors = \"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country = country.sample(frac = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_cal = country.date.max() - relativedelta(months=6)\n",
    "start_cal = end_cal - relativedelta(months=18)   # 18 meses de training\n",
    "\n",
    "country_cal = country.loc[country['date'] >= start_cal]\n",
    "country_cal = country_cal.loc[country_cal['date'] <= end_cal]\n",
    "print(country_cal.date.min(),country_cal.date.max())\n",
    "\n",
    "# |||||||||| Cargo Freq, recency, my_recency,ect - IPT - sales  ||||||||||\n",
    "summary_cal = preprocesing(country_cal)  # le paso toda la data para que fite bien\n",
    "\n",
    "past_end = (end_cal - relativedelta(months = 3))\n",
    "\n",
    "analisis_users = country.loc[(country['date'] > past_end) & (country['date'] <= end_cal)][\"cust\"] # defino este cohort como los usuarios que estuvieorn en los ultimos 3 meses\n",
    "country_cal = country_cal.loc[country_cal.cust.isin(analisis_users.values)] # me que con los que entraron en los ultimos 3 meses\n",
    "\n",
    "summary_cal = summary_cal.loc[summary_cal.cust.isin(analisis_users.values)] # me quedo con los usuarios de analisis\n",
    "\n",
    "# |||||||||| Cargo Porcentage de uso por categoria  ||||||||||\n",
    "vectorizer = CountVectorizer(token_pattern=r'[0-9].*',                         \n",
    "                             lowercase=False)\n",
    "\n",
    "# Creo el count vecrtoirzer y lo guardo en S3\n",
    "vectorizer.fit([str(int(x)) for x in country_cal[\"CATEG_L1\"].unique() if (x==x)])  # saco los nan para que el vectorizer capte todo\n",
    "joblib.dump(vectorizer, \"countVectorizer.sav\")\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file(\"countVectorizer.sav\", \"fda-labs\", s3_path + \"countVectorizer_L1.sav\")\n",
    "\n",
    "summary_cal = pd.merge(summary_cal, L_porcentage(country_cal[[\"cust\",\"CATEG_L1\"]],  # le paso solo lo que necesita usar\n",
    "                                                 vectorizer,\"CATEG_L1\"),on=\"cust\",how = \"left\")\n",
    "\n",
    "# |||||||||| Le asigno el LTV que tenia en los ultimos  ||||||||||\n",
    "past_inic = (end_cal - relativedelta(months = 6))\n",
    "past_end = (end_cal - relativedelta(months = 3))\n",
    "\n",
    "country_eval = country.loc[(country['date'] > past_end) & (country['date'] <= end_cal)] # defino este cohort como los usuarios que estuvieorn en los ultimos 3 meses\n",
    "country_eval = country.loc[(country['date'] > past_inic) & (country['date'] <= end_cal) &\n",
    "                          country.cust.isin(country_eval.cust.values)] # tomo la plata que generaron en 6 meses\n",
    "\n",
    "summary_cal = pd.merge(summary_cal,\n",
    "                       quintil_past(country_eval),\n",
    "                      on = \"cust\", how = \"left\").fillna(0) # si no estaba, entonces es quintil ultimo\n",
    "\n",
    "# |||||||||| Agrego locations ||||||||||\n",
    "locations = locations[[\"CUS_CUST_ID\",\"SHP_ADD_STATE_NAME_R\"]]\n",
    "locations.columns = [\"cust\",\"SHP_ADD_STATE_NAME_R\"]\n",
    "summary_cal = pd.merge(summary_cal,\n",
    "                       locations,\n",
    "                      on = \"cust\", how = \"left\").fillna(\"estado de méxico\") # si no estaba, entonces es quintil ultimo\n",
    "\n",
    "locations_ = locations.SHP_ADD_STATE_NAME_R.value_counts()[:31].index  # top ciudades\n",
    "aux = pd.get_dummies(summary_cal.SHP_ADD_STATE_NAME_R).drop([\"tlaxcala\"],axis = 1)\n",
    "# agrego columnas en caso que falten\n",
    "for loc in locations_:\n",
    "    if(loc not in aux.columns):\n",
    "        aux[loc] = 0\n",
    "        \n",
    "summary_cal = pd.concat([summary_cal,aux],axis = 1).drop([\"SHP_ADD_STATE_NAME_R\"],axis = 1)\n",
    "\n",
    "# |||||||||| Agrego datos de la ultima compra ||||||||||\n",
    "summary_cal = month_data_size(summary_cal,end_cal,country_cal)\n",
    "summary_cal = month_data_sales(summary_cal,end_cal,country_cal)\n",
    "summary_cal = month_data_recency(summary_cal,end_cal,country_cal)\n",
    "\n",
    "# |||||||||| Agrego datos de las visitas ||||||||||\n",
    "visitas = pd.read_csv(\"s3://fda-labs/ltv-ml/ML/MLM/Datasets/Visitas_train.csv\",sep = \"|\")\n",
    "visitas[\"recency_date\"] = pd.to_datetime(visitas[\"recency_date\"],infer_datetime_format=True,errors = \"coerce\")\n",
    "visitas[\"recency_date\"] = (end_cal  - visitas[\"recency_date\"])/np.timedelta64(1, \"D\")\n",
    "visitas[\"first_date\"] = pd.to_datetime(visitas[\"first_date\"],infer_datetime_format=True,errors = \"coerce\")\n",
    "visitas[\"first_date\"] = (end_cal  - visitas[\"first_date\"])/np.timedelta64(1, \"D\")\n",
    "cols = cols = [x for x in visitas.columns if(\"L1\" in x)]\n",
    "visitas[\"L1_tot\"] = visitas[cols].sum(axis = 1)\n",
    "\n",
    "summary_cal = pd.merge(summary_cal,visitas,on=\"cust\",how=\"left\")\n",
    "summary_cal[\"recency_date\"] = summary_cal[\"recency_date\"].fillna(-1)\n",
    "summary_cal[\"first_date\"] = summary_cal[\"first_date\"].fillna(-1)\n",
    "summary_cal = summary_cal.fillna(0)\n",
    "\n",
    "# agrego visitas segunda\n",
    "visitas = pd.read_csv(\"s3://fda-labs/ltv-ml/ML/MLM/Datasets/Visitas_train2.csv\",sep = \"|\")\n",
    "visitas[\"recency_date\"] = pd.to_datetime(visitas[\"recency_date\"],infer_datetime_format=True,errors = \"coerce\")\n",
    "visitas[\"recency_date\"] = (end_cal  - visitas[\"recency_date\"])/np.timedelta64(1, \"D\")\n",
    "visitas[\"first_date\"] = pd.to_datetime(visitas[\"first_date\"],infer_datetime_format=True,errors = \"coerce\")\n",
    "visitas[\"first_date\"] = (end_cal  - visitas[\"first_date\"])/np.timedelta64(1, \"D\")\n",
    "cols = cols = [x for x in visitas.columns if(\"L1\" in x)]\n",
    "visitas[\"L1_tot\"] = visitas[cols].sum(axis = 1)\n",
    "\n",
    "visitas.columns = [x+\"_2\" if(x != \"cust\") else x for x in visitas.columns]\n",
    "\n",
    "summary_cal = pd.merge(summary_cal,visitas,on=\"cust\",how=\"left\")\n",
    "summary_cal[\"recency_date_2\"] = summary_cal[\"recency_date_2\"].fillna(-1)\n",
    "summary_cal[\"first_date_2\"] = summary_cal[\"first_date_2\"].fillna(-1)\n",
    "summary_cal = summary_cal.fillna(0)\n",
    "\n",
    "# |||||||||| Agrego datos de comportamiento en compras ||||||||||\n",
    "compras = pd.read_csv(\"s3://fda-labs/ltv-ml/ML/MLM/Datasets/Tipos_compras_train.csv\",sep = \"|\")\n",
    "compras_cols = [\"CANTIDAD_COMPRAS_FREE_SHIPPING\", \"CANTIDAD_COMPRAS_NO_ENVIOS\",\"CANTIDAD_COMPRAS_NO_CARRITO\",\n",
    "\"CANTIDAD_COMPRAS_ENVIO_FULLFILMENT\",\"CANTIDAD_COMPRAS_ENVIO_DROP_OFF\",\"CANTIDAD_COMPRAS_ENVIO_EXPRESS\",\n",
    "\"CANTIDAD_COMPRAS_ENVIO_EXPRESS_DOMICILIO\",\"CANTIDAD_COMPRAS_ENVIO_EXPRESS_RETIRO_SUCURSAL\",\"CANTIDAD_COMPRAS_ITEMS_NUEVOS\"]\n",
    "\n",
    "compras = compras[[\"CUS_CUST_ID\"]+compras_cols]\n",
    "compras.columns = [\"cust\"]+ compras_cols\n",
    "summary_cal = pd.merge(summary_cal,compras,on=\"cust\",how=\"left\").fillna(0)\n",
    "\n",
    "# |||||||||| Agrego datos de quejas en CX ||||||||||\n",
    "claims = pd.read_csv(\"s3://fda-labs/ltv-ml/ML/MLM/Datasets/Claims_train.csv\",sep = \"|\")\n",
    "claims = claims.groupby(\"CUS_CUST_ID\").sum().reset_index()\n",
    "\n",
    "claims.columns = [\"cust\" if(x == \"CUS_CUST_ID\") else x for x in claims.columns]\n",
    "summary_cal = pd.merge(summary_cal,\n",
    "                       claims,\n",
    "                      on = \"cust\", how = \"left\").fillna(0) # si no estaba, entonces es quintil ultimo\n",
    "\n",
    "# |||||||||| Lag 12 meses ||||||||||\n",
    "summary_cal = month_lag_12(summary_cal,end_cal,country)\n",
    "\n",
    "# |||||||||| Hot sale info ||||||||||\n",
    "near_hot = nearest_date([get_real_date(end_cal,datetime.date(end_cal.year,5,28)),\n",
    "              get_real_date(end_cal,datetime.date(end_cal.year,7,16)),\n",
    "              get_real_date(end_cal,datetime.date(end_cal.year,12,20))],end_cal)\n",
    "\n",
    "summary_cal = last_hot(summary_cal,near_hot,country)\n",
    "\n",
    "# |||||||||| Data weekly durante 3 meses ||||||||||\n",
    "for i in range(12):\n",
    "    print(\"alive\")\n",
    "    fin = end_cal - relativedelta(weeks = i)\n",
    "    study = country_eval.loc[(country_eval.date <= fin) & (country_eval.date >= (fin - relativedelta(weeks = 1)))]\n",
    "    study = study.drop_duplicates(subset = [\"cust\",\"date\"]).groupby(\"cust\").size().reset_index(name = \"size_\"+str(i)+\"_weekly\")\n",
    "    summary_cal = pd.merge(summary_cal,study,on=\"cust\",how = \"left\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target\n",
    "end_eval = end_cal + relativedelta(months=3)\n",
    "\n",
    "country_eval = country.loc[(country['date'] > end_cal) & (country['date'] <= end_eval)]\n",
    "\n",
    "aux_plata = country_eval[[\"cust\",\"sales\"]].groupby(\"cust\")[\"sales\"].agg([\"sum\"]).reset_index()\n",
    "aux_transac = country_eval.set_index(\"date\").to_period(\"W\").to_timestamp().reset_index().drop_duplicates(\n",
    "    subset = [\"date\",\"cust\"]).groupby(\"cust\")[\"sales\"].agg([\"size\"]).reset_index()\n",
    "\n",
    "aux_plata.columns = [\"cust\",\"money_eval\"]\n",
    "aux_transac.columns = [\"cust\",\"frequency_eval\"]\n",
    "\n",
    "aux_plata = pd.merge(aux_plata,aux_transac,on=\"cust\",how = \"outer\").fillna(0)\n",
    "aux_plata[\"engage_ltv\"] = aux_plata[\"money_eval\"]*aux_plata[\"frequency_eval\"]\n",
    "\n",
    "summary_cal = pd.merge(summary_cal,aux_plata,on=\"cust\",how = \"left\").fillna(0)\n",
    "summary_cal = summary_cal.sort_values(by = \"engage_ltv\",ascending = False)\n",
    "summary_cal[\"quintil_true\"] = asigno_quintiles_4(summary_cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cal.to_pickle(\"cal.pkl\")\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file(\"cal.pkl\", \"fda-labs\", s3_path+\"summary_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_1 = []\n",
    "for q in summary_cal.quintil_true.unique():\n",
    "    l_1.append(summary_cal.loc[summary_cal.quintil_true == q].sample(frac=0.1))\n",
    "\n",
    "_write_dataframe_to_csv_on_s3(pd.concat(l_1),\"s3://fda-labs/\"+s3_path+\"sample_summary_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora lo mismo para el set de testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-20 00:00:00 2019-04-20 00:00:00\n",
      "Preproc para pareto\n",
      "Recency frequency\n",
      "fiteo\n",
      "npred\n",
      "p_alive\n",
      "fit\n",
      "IPT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [04:35<00:00,  6.89s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:04<00:00,  8.43it/s]\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:29: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:29: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:29: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:29: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:29: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:29: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:29: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:29: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:29: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'cust', 'sales', 'CATEG_L1', 'CATEG_L2', 'CATEG_L3'], dtype='object')\n",
      "Index(['cust', 'sales'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# (año 0 ~ año 1.5) -- (año 1.5 ~ año 1.8)   : train -- test\n",
    "#                      (año 0 ~ año 1.5) -- (año 1.5 ~ año 1.8)   : idem para test final pero con 1 año de overlap\n",
    "\n",
    "end_cal = country.date.max() - relativedelta(months=3)  # me guardo 1 año para training\n",
    "start_cal = end_cal - relativedelta(months=18)\n",
    "\n",
    "country_cal = country.loc[country['date'] >= start_cal]\n",
    "country_cal = country_cal.loc[country_cal['date'] <= end_cal]\n",
    "\n",
    "print(country_cal.date.min(),country_cal.date.max())\n",
    "\n",
    "# |||||||||| Cargo Freq, recency, my_recency,ect - IPT - sales  ||||||||||\n",
    "summary_cal = preprocesing(country_cal)\n",
    "\n",
    "past_end = (end_cal - relativedelta(months = 3))\n",
    "\n",
    "analisis_users = country.loc[(country['date'] > past_end) & (country['date'] <= end_cal)][\"cust\"] # defino este cohort como los usuarios que estuvieorn en los ultimos 3 meses\n",
    "country_cal = country_cal.loc[country_cal.cust.isin(analisis_users.values)] # me que con los que entraron en los ultimos 3 meses\n",
    "\n",
    "summary_cal = summary_cal.loc[summary_cal.cust.isin(analisis_users.values)] # me quedo con los usuarios de analisis\n",
    "\n",
    "# |||||||||| Cargo Porcentage de uso por categoria  ||||||||||\n",
    "summary_cal = pd.merge(summary_cal, L_porcentage(country_cal[[\"cust\",\"CATEG_L1\"]],  # le paso solo lo que necesita usar\n",
    "                                                 vectorizer,\"CATEG_L1\"),on=\"cust\",how = \"left\")\n",
    "\n",
    "# |||||||||| Le asigno el LTV que tenia en los ultimos  ||||||||||\n",
    "past_inic = (end_cal - relativedelta(months = 6))\n",
    "past_end = (end_cal - relativedelta(months = 3))\n",
    "\n",
    "country_eval = country.loc[(country['date'] > past_end) & (country['date'] <= end_cal)] # defino este cohort como los usuarios que estuvieorn en los ultimos 3 meses\n",
    "country_eval = country.loc[(country['date'] > past_inic) & (country['date'] <= end_cal) &\n",
    "                          country.cust.isin(country_eval.cust.values)] # tomo la plata que generaron en 6 meses\n",
    "\n",
    "summary_cal = pd.merge(summary_cal,\n",
    "                       quintil_past(country_eval),\n",
    "                      on = \"cust\", how = \"left\").fillna(0) # si no estaba, entonces es quintil ultimo\n",
    "\n",
    "# |||||||||| Agrego locations ||||||||||\n",
    "\n",
    "summary_cal = pd.merge(summary_cal,\n",
    "                       locations,    # ya tiene bien el nombte :)\n",
    "                       on=\"cust\", how = \"left\").fillna(\"estado de méxico\") # si no estaba, entonces es quintil ultimo\n",
    "\n",
    "aux = pd.get_dummies(summary_cal.SHP_ADD_STATE_NAME_R).drop([\"tlaxcala\"],axis = 1)\n",
    "# agrego columnas en caso que falten\n",
    "for loc in locations_:\n",
    "    if(loc not in aux.columns):\n",
    "        aux[loc] = 0\n",
    "        \n",
    "summary_cal = pd.concat([summary_cal,aux],axis = 1).drop([\"SHP_ADD_STATE_NAME_R\"],axis = 1)\n",
    "\n",
    "# |||||||||| Agrego datos de la ultima compra ||||||||||\n",
    "summary_cal = month_data_size(summary_cal,end_cal,country_cal)\n",
    "summary_cal = month_data_sales(summary_cal,end_cal,country_cal)\n",
    "summary_cal = month_data_recency(summary_cal,end_cal,country_cal)\n",
    "\n",
    "# |||||||||| Agrego datos de las visitas ||||||||||\n",
    "visitas = pd.read_csv(\"s3://fda-labs/ltv-ml/ML/MLM/Datasets/Visitas_test.csv\",sep = \"|\")\n",
    "visitas[\"recency_date\"] = pd.to_datetime(visitas[\"recency_date\"],infer_datetime_format=True,errors = \"coerce\")\n",
    "visitas[\"recency_date\"] = (end_cal  - visitas[\"recency_date\"])/np.timedelta64(1, \"D\")\n",
    "visitas[\"first_date\"] = pd.to_datetime(visitas[\"first_date\"],infer_datetime_format=True,errors = \"coerce\")\n",
    "visitas[\"first_date\"] = (end_cal  - visitas[\"first_date\"])/np.timedelta64(1, \"D\")\n",
    "cols = cols = [x for x in visitas.columns if(\"L1\" in x)]\n",
    "visitas[\"L1_tot\"] = visitas[cols].sum(axis = 1)\n",
    "\n",
    "summary_cal = pd.merge(summary_cal,visitas,on=\"cust\",how=\"left\")\n",
    "summary_cal[\"recency_date\"] = summary_cal[\"recency_date\"].fillna(-1)\n",
    "summary_cal[\"first_date\"] = summary_cal[\"first_date\"].fillna(-1)\n",
    "summary_cal = summary_cal.fillna(0)\n",
    "\n",
    "# guardo el segundo archivo\n",
    "visitas = pd.read_csv(\"s3://fda-labs/ltv-ml/ML/MLM/Datasets/Visitas_test2.csv\",sep = \"|\")\n",
    "visitas[\"recency_date\"] = pd.to_datetime(visitas[\"recency_date\"],infer_datetime_format=True,errors = \"coerce\")\n",
    "visitas[\"recency_date\"] = (end_cal  - visitas[\"recency_date\"])/np.timedelta64(1, \"D\")\n",
    "visitas[\"first_date\"] = pd.to_datetime(visitas[\"first_date\"],infer_datetime_format=True,errors = \"coerce\")\n",
    "visitas[\"first_date\"] = (end_cal  - visitas[\"first_date\"])/np.timedelta64(1, \"D\")\n",
    "cols = cols = [x for x in visitas.columns if(\"L1\" in x)]\n",
    "visitas[\"L1_tot\"] = visitas[cols].sum(axis = 1)\n",
    "\n",
    "visitas.columns = [x+\"_2\" if(x != \"cust\") else x for x in visitas.columns]\n",
    "\n",
    "summary_cal = pd.merge(summary_cal,visitas,on=\"cust\",how=\"left\")\n",
    "summary_cal[\"recency_date_2\"] = summary_cal[\"recency_date_2\"].fillna(-1)\n",
    "summary_cal[\"first_date_2\"] = summary_cal[\"first_date_2\"].fillna(-1)\n",
    "summary_cal = summary_cal.fillna(0)\n",
    "\n",
    "# |||||||||| Agrego datos de comportamiento en compras ||||||||||\n",
    "compras = pd.read_csv(\"s3://fda-labs/ltv-ml/ML/MLM/Datasets/Tipos_compras_test.csv\",sep = \"|\")\n",
    "compras_cols = [\"CANTIDAD_COMPRAS_FREE_SHIPPING\", \"CANTIDAD_COMPRAS_NO_ENVIOS\",\"CANTIDAD_COMPRAS_NO_CARRITO\",\n",
    "\"CANTIDAD_COMPRAS_ENVIO_FULLFILMENT\",\"CANTIDAD_COMPRAS_ENVIO_DROP_OFF\",\"CANTIDAD_COMPRAS_ENVIO_EXPRESS\",\n",
    "\"CANTIDAD_COMPRAS_ENVIO_EXPRESS_DOMICILIO\",\"CANTIDAD_COMPRAS_ENVIO_EXPRESS_RETIRO_SUCURSAL\",\"CANTIDAD_COMPRAS_ITEMS_NUEVOS\"]\n",
    "\n",
    "compras = compras[[\"CUS_CUST_ID\"]+compras_cols]\n",
    "compras.columns = [\"cust\"]+ compras_cols\n",
    "summary_cal = pd.merge(summary_cal,compras,on=\"cust\",how=\"left\").fillna(0)\n",
    "\n",
    "# |||||||||| Agrego datos de quejas en CX ||||||||||\n",
    "claims = pd.read_csv(\"s3://fda-labs/ltv-ml/ML/MLM/Datasets/Claims_test.csv\",sep = \"|\")\n",
    "claims = claims.groupby(\"CUS_CUST_ID\").sum().reset_index()\n",
    "\n",
    "claims.columns = [\"cust\" if(x == \"CUS_CUST_ID\") else x for x in claims.columns]\n",
    "summary_cal = pd.merge(summary_cal,\n",
    "                       claims,\n",
    "                      on = \"cust\", how = \"left\").fillna(0) # si no estaba, entonces es quintil ultimo\n",
    "\n",
    "# |||||||||| Lag 12 meses ||||||||||\n",
    "summary_cal = month_lag_12(summary_cal,end_cal,country)\n",
    "\n",
    "# |||||||||| Hot sale info ||||||||||\n",
    "near_hot = nearest_date([get_real_date(end_cal,datetime.date(end_cal.year,5,28)),\n",
    "              get_real_date(end_cal,datetime.date(end_cal.year,7,16)),\n",
    "              get_real_date(end_cal,datetime.date(end_cal.year,12,20))],end_cal)\n",
    "\n",
    "summary_cal = last_hot(summary_cal,near_hot,country)\n",
    "\n",
    "# |||||||||| Data weekly durante 3 meses ||||||||||\n",
    "for i in range(12):\n",
    "    print(\"alive\")\n",
    "    fin = end_cal - relativedelta(weeks = i)\n",
    "    study = country_eval.loc[(country_eval.date <= fin) & (country_eval.date >= (fin - relativedelta(weeks = 1)))]\n",
    "    study = study.drop_duplicates(subset = [\"cust\",\"date\"]).groupby(\"cust\").size().reset_index(name = \"size_\"+str(i)+\"_weekly\")\n",
    "    summary_cal = pd.merge(summary_cal,study,on=\"cust\",how = \"left\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target\n",
    "end_eval = end_cal + relativedelta(months=3)\n",
    "\n",
    "country_eval = country.loc[(country['date'] > end_cal) & (country['date'] <= end_eval)]\n",
    "\n",
    "aux_plata = country_eval[[\"cust\",\"sales\"]].groupby(\"cust\")[\"sales\"].agg([\"sum\"]).reset_index()\n",
    "aux_transac = country_eval.set_index(\"date\").to_period(\"W\").to_timestamp().reset_index().drop_duplicates(\n",
    "    subset = [\"date\",\"cust\"]).groupby(\"cust\")[\"sales\"].agg([\"size\"]).reset_index()\n",
    "\n",
    "aux_plata.columns = [\"cust\",\"money_eval\"]\n",
    "aux_transac.columns = [\"cust\",\"frequency_eval\"]\n",
    "\n",
    "aux_plata = pd.merge(aux_plata,aux_transac,on=\"cust\",how = \"outer\").fillna(0)\n",
    "aux_plata[\"engage_ltv\"] = aux_plata[\"money_eval\"]*aux_plata[\"frequency_eval\"]\n",
    "\n",
    "summary_cal = pd.merge(summary_cal,aux_plata,on=\"cust\",how = \"left\").fillna(0)\n",
    "summary_cal = summary_cal.sort_values(by = \"engage_ltv\",ascending = False)\n",
    "summary_cal[\"quintil_true\"] = asigno_quintiles_4(summary_cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cal.to_pickle(\"cal.pkl\")\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file(\"cal.pkl\", \"fda-labs\", s3_path+\"summary_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle_csv(path):\n",
    "    import boto3\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket = path.split('/')[2]\n",
    "    resto = '/'.join(path.split('/')[3:])\n",
    "    s3.download_file(bucket, resto,\"aux.pkl\")\n",
    "    pp = pd.read_pickle(\"aux.pkl\")\n",
    "    os.remove(\"aux.pkl\")\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cal = read_pickle_csv(\"s3://fda-labs/\"+s3_path+\"summary_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_eval = country.loc[(country.date <= end_cal) & (country.date >= (end_cal - relativedelta(weeks = 12)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_cal = summary_cal.date_max.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alive\n",
      "alive\n",
      "alive\n",
      "alive\n",
      "alive\n",
      "alive\n",
      "alive\n",
      "alive\n",
      "alive\n",
      "alive\n",
      "alive\n",
      "alive\n"
     ]
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    print(\"alive\")\n",
    "    fin = end_cal - relativedelta(weeks = i)\n",
    "    study = country_eval.loc[(country_eval.date <= fin) & (country_eval.date >= (fin - relativedelta(weeks = 1)))]\n",
    "    study = study.drop_duplicates(subset = [\"cust\",\"date\"]).groupby(\"cust\").size().reset_index(name = \"size_\"+str(i)+\"_weekly\")\n",
    "    summary_cal = pd.merge(summary_cal,study,on=\"cust\",how = \"left\").fillna(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLAG_Claim_other</th>\n",
       "      <th>FLAG_CLAIM_CLOSED</th>\n",
       "      <th>TIEMPO_RESOLUCION</th>\n",
       "      <th>size_lag_12</th>\n",
       "      <th>size_last_hot</th>\n",
       "      <th>size_0_weekly</th>\n",
       "      <th>size_1_weekly</th>\n",
       "      <th>size_2_weekly</th>\n",
       "      <th>size_3_weekly</th>\n",
       "      <th>size_4_weekly</th>\n",
       "      <th>size_5_weekly</th>\n",
       "      <th>size_6_weekly</th>\n",
       "      <th>size_7_weekly</th>\n",
       "      <th>size_8_weekly</th>\n",
       "      <th>size_9_weekly</th>\n",
       "      <th>size_10_weekly</th>\n",
       "      <th>size_11_weekly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FLAG_Claim_other  FLAG_CLAIM_CLOSED  TIEMPO_RESOLUCION  size_lag_12  \\\n",
       "0               0.0                1.0                0.0          0.0   \n",
       "1               0.0                1.0                0.0         72.0   \n",
       "2               0.0               32.0              123.0          3.0   \n",
       "3               0.0                0.0                0.0          0.0   \n",
       "4               0.0                0.0                0.0          3.0   \n",
       "5               0.0                1.0                0.0         10.0   \n",
       "6               0.0                2.0               17.0          0.0   \n",
       "7               0.0                2.0                6.0          1.0   \n",
       "8               0.0                2.0               71.0         12.0   \n",
       "9               0.0                0.0                0.0         12.0   \n",
       "\n",
       "   size_last_hot  size_0_weekly  size_1_weekly  size_2_weekly  size_3_weekly  \\\n",
       "0            0.0            5.0            4.0            4.0            4.0   \n",
       "1            3.0            6.0            7.0            8.0            7.0   \n",
       "2            9.0            7.0            7.0            6.0            7.0   \n",
       "3            0.0            7.0            6.0            3.0            2.0   \n",
       "4            5.0            4.0            5.0            1.0            4.0   \n",
       "5            0.0            1.0            5.0            3.0            3.0   \n",
       "6            0.0            0.0            0.0            0.0            4.0   \n",
       "7            3.0            3.0            3.0            6.0            4.0   \n",
       "8            2.0            2.0            1.0            1.0            1.0   \n",
       "9            6.0            0.0            2.0            2.0            1.0   \n",
       "\n",
       "   size_4_weekly  size_5_weekly  size_6_weekly  size_7_weekly  size_8_weekly  \\\n",
       "0            4.0            3.0            4.0            2.0            1.0   \n",
       "1            6.0            7.0            7.0            4.0            7.0   \n",
       "2            8.0            7.0            7.0            6.0            5.0   \n",
       "3            4.0            3.0            2.0            3.0            0.0   \n",
       "4            6.0            5.0            1.0            0.0            0.0   \n",
       "5            2.0            4.0            2.0            2.0            1.0   \n",
       "6            3.0            4.0            3.0            1.0            1.0   \n",
       "7            7.0            1.0            5.0            1.0            2.0   \n",
       "8            0.0            1.0            0.0            2.0            1.0   \n",
       "9            1.0            3.0            2.0            1.0            0.0   \n",
       "\n",
       "   size_9_weekly  size_10_weekly  size_11_weekly  \n",
       "0            0.0             0.0             0.0  \n",
       "1            5.0             5.0             6.0  \n",
       "2            6.0             7.0             5.0  \n",
       "3            2.0             3.0             5.0  \n",
       "4            3.0             5.0             5.0  \n",
       "5            0.0             0.0             1.0  \n",
       "6            0.0             0.0             0.0  \n",
       "7            4.0             1.0             1.0  \n",
       "8            1.0             1.0             1.0  \n",
       "9            2.0             3.0             1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_cal[summary_cal.columns[180:]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-04-20 00:00:00')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
