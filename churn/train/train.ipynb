{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting\")\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import boto3\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error,average_precision_score,mean_squared_error\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale = 1)\n",
    "\n",
    "import warnings\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from multiprocessing import  cpu_count,Pool\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from lifetimes import ModifiedBetaGeoFitter\n",
    "from lifetimes import GammaGammaFitter\n",
    "from dateutil.relativedelta import relativedelta \n",
    "from lifetimes.utils import summary_data_from_transaction_data\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "import gc\n",
    "from sklearn.utils import class_weight\n",
    "from melitk.fda import workspace\n",
    "import time\n",
    "import shutil\n",
    "from base64 import b64decode\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.expanduser(\".\")))\n",
    "sys.path.append(os.path.dirname(os.path.expanduser(\"..\")))\n",
    "sys.path.append(os.path.dirname(os.path.expanduser(\"../defines\")))\n",
    "from defines import *\n",
    "\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(\"fury-data-apps\", \"marketing-utils/pzenone/utils.py\",\"utils.py\")\n",
    "import utils\n",
    "\n",
    "target_col = \"churn\"\n",
    "\n",
    "TRAIN = True\n",
    "\n",
    "from preprocessing import *\n",
    "from train_utils import train_combinations,simple_fit,foo_evaluation_classifier,foo_model_classifier,foo_predict_classifier,anti_dummies,caster,picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cliente BigQuery\n",
    "AUTH_BIGQUERY = b64decode(os.environ['SECRET_MODELLING_BIGQUERY'])\n",
    "\n",
    "from google_cloud import BigQuery\n",
    "from google_cloud import Storage\n",
    "\n",
    "bq = BigQuery(AUTH_BIGQUERY)\n",
    "bqs = Storage(AUTH_BIGQUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levanto todos los paises y calculo con BTYD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(summary_cal):\n",
    "\n",
    "    # Steps para preprocesar\n",
    "    steps = []\n",
    "    \n",
    "    if (PAIS == 'MLB' or PAIS == 'MLA'):\n",
    "        steps = steps + [prepaid,asset_mgm,install_mp]\n",
    "    if (PAIS == 'MLB'):\n",
    "        steps = steps + [reg_data]\n",
    "        \n",
    "    steps = steps + [orders_frequencies,IPT,locations,demograficos, compras_shipping, target, tarjetas, visitas_short,\n",
    "         install_ml, payments, sellers, asp_pareto]\n",
    "\n",
    "    \n",
    "    for func in steps:\n",
    "        start = time.time()\n",
    "        bqs.print_gcps(\"--------------------\",bq_log_path=bq_log_path,silent = True)\n",
    "        bqs.print_gcps(\"Start time:\",datetime.datetime.now(),bq_log_path=bq_log_path,silent = True)\n",
    "        bqs.print_gcps(\"Starting:\",func.__name__,\"Summary size:\",summary_cal.shape,bq_log_path=bq_log_path)\n",
    "        summary_cal = func(bqs, gcps_path_in,summary_cal,\"test\")\n",
    "        end = time.time()\n",
    "        bqs.print_gcps(\"tiempo en resolver:\", end-start,bq_log_path=bq_log_path,silent = True)\n",
    "    \n",
    "    return summary_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all(bqs,gcps_path_in):\n",
    "    \n",
    "    summary_cal = read_gfs_parquet(gcps_path_in, bqs, 'summary_cal')\n",
    "    summary_cal = preprocess(summary_cal)\n",
    "        \n",
    "    out_file = f\"summary_train.pkl\"\n",
    "    summary_cal.to_pickle(out_file)\n",
    "    \n",
    "    bqs.upload_file(out_file,gcps_path_in + out_file)\n",
    "    \n",
    "    return summary_cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparo_datos(summary_cal):\n",
    "    \n",
    "    numeric_dtypes = [np.dtype('float32'),np.dtype('float64'), np.dtype('int32'),np.dtype('int64')]\n",
    "    \n",
    "    total_cols = list(summary_cal.columns)\n",
    "    if('CUS_CUST_ID' in total_cols): total_cols.remove('CUS_CUST_ID')\n",
    "    \n",
    "    # Evitar filtrar la variable target\n",
    "    if(target_col in total_cols): total_cols.remove(target_col)\n",
    "    \n",
    "    # Eliminado de la query de Target, pero si lo vuelven a usar esta \n",
    "    if('GMV_TARGET' in total_cols): total_cols.remove('GMV_TARGET')\n",
    "    \n",
    "    # Filtro APP_INSTALL (Uso en Churn) \n",
    "    if('APP_INSTALL' in total_cols): total_cols.remove('APP_INSTALL')\n",
    "    \n",
    "    train_cols_nocat = [x for x in total_cols if(summary_cal.dtypes[x] in numeric_dtypes)]\n",
    "    \n",
    "    cates = [x for x in total_cols if(summary_cal.dtypes[x] not in numeric_dtypes)]\n",
    "\n",
    "    for x in train_cols_nocat:\n",
    "        summary_cal[x] = summary_cal[x].astype(float)\n",
    "    for x in cates:\n",
    "        summary_cal[x] = summary_cal[x].astype(str)\n",
    "        \n",
    "    return summary_cal,cates,train_cols_nocat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess parquet files\n",
    "summary_cal = preprocess_all(bqs,gcps_path_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento Modelo de seleccion de Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cal = pd.read_pickle(\"summary_train.pkl\")\n",
    "\n",
    "# Definidas en Defines.py\n",
    "summary_cal = summary_cal[[x for x in summary_cal.columns if x not in EXCLUDE_VARIABLES[PAIS]]]\n",
    "summary_cal.loc[summary_cal.freq_180D < 15] # Filtro por las dudas lo que se filtro en la pivot en el ETL\n",
    "summary_cal[target_col] = np.where((summary_cal[\"freq_180D\"]/2).astype(int) >  summary_cal[\"frequency_eval\"] ,1,0)\n",
    "\n",
    "#CUS_CUST_ID no es relevante, y GMV_TARGET - frequency_eval producen Data Leakage\n",
    "train_cols = [x for x in summary_cal.columns if x not in  ['CUS_CUST_ID','GMV_TARGET','frequency_eval']]\n",
    "\n",
    "summary_cal,cates,train_cols_nocat = preparo_datos(summary_cal[train_cols])\n",
    "train, valid= train_test_split(summary_cal,test_size=0.15,random_state=0,stratify=summary_cal[target_col].values)\n",
    "train, test= train_test_split(train,test_size=0.15,random_state=0,stratify=train[target_col].values)\n",
    "\n",
    "# Para debuggear modelo\n",
    "train.to_pickle(\"train.pkl\")\n",
    "test.to_pickle(\"test.pkl\")\n",
    "test.to_pickle(\"valid.pkl\")\n",
    "\n",
    "bqs.upload_file(\"train.pkl\", gcps_path_out + \"train.pkl\")\n",
    "bqs.upload_file(\"test.pkl\", gcps_path_out + \"test.pkl\")\n",
    "bqs.upload_file(\"valid.pkl\", gcps_path_out + \"valid.pkl\")\n",
    "\n",
    "bqs.print_gcps(\"Churn Rate:\\n\",summary_cal[target_col].value_counts()/summary_cal.shape[0],bq_log_path=bq_log_path)\n",
    "proporcion = summary_cal[target_col].value_counts()[1]/summary_cal.shape[0]  # Porpocion que churnea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ =  np.c_[train[train_cols_nocat].values,train[cates].values]\n",
    "X_valid_ =  np.c_[valid[train_cols_nocat].values,valid[cates].values]\n",
    "X_test =  np.c_[test[train_cols_nocat].values,test[cates].values]\n",
    "\n",
    "y_train_ =  np.array(train[target_col])\n",
    "y_valid_ =  np.array(valid[target_col])\n",
    "y_test =  np.array(test[target_col])  \n",
    "\n",
    "# indices de las categorias\n",
    "cates_ind = [X_train_.shape[1] - len(cates) + i for i in range(len(cates))]\n",
    "\n",
    "# Entreno    \n",
    "basic_params = {\n",
    "           'use_best_model':True,\n",
    "           'loss_function': 'Logloss',\n",
    "            'eval_metric':'Logloss',\n",
    "            'verbose':False,\n",
    "            'boosting_type':\"Plain\",\n",
    "            'bootstrap_type':\"Bernoulli\",\n",
    "            'iterations':500,\n",
    "#              'class_weights':weights_l,\n",
    "#              'rsm':0.1,  # OJO!!! esto solo va si tenes mas de 100 features!,\n",
    "#             'max_ctr_complexity':2,\n",
    "           'random_state': 45\n",
    "        }\n",
    "\n",
    "hyperparams = { \n",
    "                'learning_rate':[0.01,0.05,0.1,0.2,0.3],\n",
    "                'depth': [1,3,5,7,10],                                                       \n",
    "                'l2_leaf_reg': [1,10,50,100,150,300,500,750,1000],\n",
    "                'border_count': [5,10,30,60,100,150,200]\n",
    "            }\n",
    "\n",
    "d_types = { 'depth': int,\n",
    "           'l2_leaf_reg': int,\n",
    "           'border_count':int\n",
    "          }\n",
    "\n",
    "# indices de las categorias\n",
    "cates_ind = [X_train_.shape[1] - len(cates) + i for i in range(len(cates))]  \n",
    "\n",
    "\n",
    "params = train_combinations(X_train_,X_valid_,y_train_,y_valid_,\"average_precision\",foo_predict_classifier,\n",
    "                              foo_model_classifier,basic_params,d_types,hyperparams,cates_ind, n_iter = SELECT_VARIABLES_STEPS)\n",
    "\n",
    "best_model =  simple_fit(params,X_train_,y_train_,X_valid_,y_valid_,foo_model_classifier,basic_params,d_types,cates_ind)\n",
    "\n",
    "error_test = foo_evaluation_classifier(y_test,foo_predict_classifier(best_model,X_test))\n",
    "error_train = foo_evaluation_classifier(y_train_,foo_predict_classifier(best_model,X_train_))\n",
    "error_validation = foo_evaluation_classifier(y_valid_,foo_predict_classifier(best_model,X_valid_))\n",
    "\n",
    "bqs.print_gcps(\"Error Test:\\n\",error_test,bq_log_path=bq_log_path)\n",
    "bqs.print_gcps(\"Error Validation:\\n\",error_validation,bq_log_path=bq_log_path)\n",
    "bqs.print_gcps(\"Error Train:\\n\",error_train,bq_log_path=bq_log_path)\n",
    "\n",
    "# Guado resultados!\n",
    "with open('model.pkl', 'wb') as fout:\n",
    "    pickle.dump(best_model, fout)\n",
    "\n",
    "# No subo el modelo porque es el entrenado con muy pocas iteraciones\n",
    "#bqs.upload_file(\"model.pkl\",gcps_path_out + \"model.pkl\")\n",
    "\n",
    "importance = picture(np.r_[X_train_,X_valid_],np.r_[y_train_,y_valid_],X_test,y_test,cates_ind,best_model,foo_model_classifier,train_cols_nocat,cates)\n",
    "\n",
    "importance.sort_values('Value', ascending = False).to_csv(\"importance_short.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrado de variables por importancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imp = pd.read_csv(f'importance_short.csv').sort_values('Value', ascending = False)\n",
    "\n",
    "max_importance = imp.iloc[1,1]\n",
    "rand_importance = imp.loc[imp.Feature == 'rand'].values[0][1]\n",
    "\n",
    "print(f\"max_importance: {max_importance}\")\n",
    "print(f\"rand_importance: {rand_importance}\")\n",
    "\n",
    "# Filtros para seleccionar las varibles relevantes para el entrenamiento completo\n",
    "mayores_a_rand = list(imp.loc[imp.Value > rand_importance].Feature)\n",
    "mayores_orden_4 = list(imp.loc[imp.Value >= (max_importance / 1000)].Feature)\n",
    "distinto_0 = list(imp.loc[imp.Value > 0.0].Feature)\n",
    "\n",
    "total_vars = [x for x in list(imp.Feature) if (x in mayores_a_rand) and (x in mayores_orden_4) and (x in distinto_0)]\n",
    "\n",
    "pd.DataFrame(total_vars, columns = ['Feature']).to_csv(f\"features_model.csv\", index = False)\n",
    "\n",
    "# Subo features del modelo N al Storage, incluidos el cluster y la columna target\n",
    "bqs.upload_file(f\"features_model.csv\", gcps_path_out + f\"features_model.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento Modelo con variables filtradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bqs.download_file(gcps_path_in + \"summary_train.pkl\", \"summary_train.pkl\")\n",
    "summary_cal = pd.read_pickle(\"summary_train.pkl\")\n",
    "summary_cal.loc[summary_cal.freq_180D < 15] # Filtro por las dudas lo que se filtro en la pivot en el ETL\n",
    "summary_cal[target_col] = np.where((summary_cal[\"freq_180D\"]/2).astype(int) >  summary_cal[\"frequency_eval\"] ,1,0)\n",
    "\n",
    "#CUS_CUST_ID no es relevante, y GMV_TARGET / frequency_eval producen Data Leakage\n",
    "train_cols = list(pd.read_csv(f\"features_model.csv\").Feature) + [target_col]\n",
    "\n",
    "\n",
    "summary_cal,cates,train_cols_nocat = preparo_datos(summary_cal[train_cols])\n",
    "train, valid= train_test_split(summary_cal,test_size=0.15,random_state=0,stratify=summary_cal[target_col].values)\n",
    "train, test= train_test_split(train,test_size=0.15,random_state=0,stratify=train[target_col].values)\n",
    "\n",
    "# Para debuggear modelo\n",
    "train.to_pickle(\"train.pkl\")\n",
    "test.to_pickle(\"test.pkl\")\n",
    "test.to_pickle(\"valid.pkl\")\n",
    "\n",
    "bqs.upload_file(\"train.pkl\", gcps_path_out + \"train.pkl\")\n",
    "bqs.upload_file(\"test.pkl\", gcps_path_out + \"test.pkl\")\n",
    "bqs.upload_file(\"valid.pkl\", gcps_path_out + \"valid.pkl\")\n",
    "\n",
    "bqs.print_gcps(\"Churn Rate:\\n\",summary_cal[target_col].value_counts()/summary_cal.shape[0],bq_log_path=bq_log_path)\n",
    "proporcion = summary_cal[target_col].value_counts()[1]/summary_cal.shape[0]  # Porpocion que churnea\n",
    "\n",
    "X_train_ =  np.c_[train[train_cols_nocat].values,train[cates].values]\n",
    "X_valid_ =  np.c_[valid[train_cols_nocat].values,valid[cates].values]\n",
    "X_test =  np.c_[test[train_cols_nocat].values,test[cates].values]\n",
    "\n",
    "y_train_ =  np.array(train[target_col])\n",
    "y_valid_ =  np.array(valid[target_col])\n",
    "y_test =  np.array(test[target_col])  \n",
    "\n",
    "cates_ind = [X_train_.shape[1] - len(cates) + i for i in range(len(cates))]  # indices de las categorias\n",
    "\n",
    "# Entreno    \n",
    "basic_params = {\n",
    "           'use_best_model':True,\n",
    "           'loss_function': 'Logloss',\n",
    "            'eval_metric':'Logloss',\n",
    "            'verbose':False,\n",
    "            'boosting_type':\"Plain\",\n",
    "            'bootstrap_type':\"Bernoulli\",\n",
    "            'iterations':500,\n",
    "#              'class_weights':weights_l,\n",
    "#              'rsm':0.1,  # OJO!!! esto solo va si tenes mas de 100 features!,\n",
    "#             'max_ctr_complexity':2,\n",
    "           'random_state': 45\n",
    "        }\n",
    "\n",
    "hyperparams = { \n",
    "                'learning_rate':[0.01,0.05,0.1,0.2,0.3],\n",
    "                'depth': [1,3,5,7,10],                                                       \n",
    "                'l2_leaf_reg': [1,10,50,100,150,300,500,750,1000],\n",
    "                'border_count': [5,10,30,60,100,150,200]\n",
    "            }\n",
    "\n",
    "d_types = { 'depth': int,\n",
    "           'l2_leaf_reg': int,\n",
    "           'border_count':int\n",
    "          }\n",
    "cates_ind = [X_train_.shape[1] - len(cates) + i for i in range(len(cates))]  # indices de las categorias\n",
    "\n",
    "params = train_combinations(X_train_,X_valid_,y_train_,y_valid_,\"average_precision\",foo_predict_classifier,\n",
    "                              foo_model_classifier,basic_params,d_types,hyperparams,cates_ind, n_iter = FULL_TRAINING_STEPS)\n",
    "\n",
    "best_model =  simple_fit(params,X_train_,y_train_,X_valid_,y_valid_,foo_model_classifier,basic_params,d_types,cates_ind)\n",
    "\n",
    "error_test = foo_evaluation_classifier(y_test,foo_predict_classifier(best_model,X_test))\n",
    "error_train = foo_evaluation_classifier(y_train_,foo_predict_classifier(best_model,X_train_))\n",
    "error_validation = foo_evaluation_classifier(y_valid_,foo_predict_classifier(best_model,X_valid_))\n",
    "\n",
    "bqs.print_gcps(\"Error Test:\\n\",error_test,bq_log_path=bq_log_path)\n",
    "bqs.print_gcps(\"Error Validation:\\n\",error_validation,bq_log_path=bq_log_path)\n",
    "bqs.print_gcps(\"Error Train:\\n\",error_train,bq_log_path=bq_log_path)\n",
    "\n",
    "# Guado resultados!\n",
    "with open('model.pkl', 'wb') as fout:\n",
    "    pickle.dump(best_model, fout)\n",
    "bqs.upload_file(\"model.pkl\",gcps_path_out + \"model.pkl\")\n",
    "\n",
    "importance = picture(np.r_[X_train_,X_valid_],np.r_[y_train_,y_valid_],X_test,y_test,cates_ind,best_model,foo_model_classifier,train_cols_nocat,cates)\n",
    "bqs.upload_file(\"lgbmImportance.png\", gcps_path_out + \"lgbmImportance.png\")\n",
    "#importance.to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Optimo - Precision Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Busco el treshold optimo. Armo curva PR idel vs test\n",
    "from sklearn.metrics import precision_score,recall_score\n",
    "\n",
    "test[\"churn_proba\"] = np.array(best_model.predict_proba(X_test)[:,1])\n",
    "valid[\"churn_proba\"] = np.array(best_model.predict_proba(X_valid_)[:,1])\n",
    "train[\"churn_proba\"] = np.array(best_model.predict_proba(X_train_)[:,1])\n",
    "\n",
    "l_ = []\n",
    "for t in np.arange(0.001,0.99,0.01):\n",
    "    l_.append({\"tresh\":t,\"precision\":precision_score(test[target_col],np.where(test.churn_proba >= t,1,0)),\n",
    "               \"recall\":recall_score(test[target_col],np.where(test.churn_proba >= t,1,0)),\n",
    "               \"cantidad\":test.loc[test.churn_proba >= t].shape[0]/test.shape[0]})\n",
    "df_tresh_test = pd.DataFrame(l_)\n",
    "\n",
    "l_ = []\n",
    "for t in np.arange(0.001,0.99,0.01):\n",
    "    l_.append({\"tresh\":t,\"precision\":precision_score(valid[target_col],np.where(valid.churn_proba >= t,1,0)),\n",
    "               \"recall\":recall_score(valid[target_col],np.where(valid.churn_proba >= t,1,0)),\n",
    "               \"cantidad\":valid.loc[valid.churn_proba >= t].shape[0]/valid.shape[0]})\n",
    "df_tresh_valid = pd.DataFrame(l_)\n",
    "\n",
    "l_ = []\n",
    "for t in np.arange(0.001,0.99,0.01):\n",
    "    l_.append({\"tresh\":t,\"precision\":precision_score(train[target_col],np.where(train.churn_proba >= t,1,0)),\n",
    "               \"recall\":recall_score(train[target_col],np.where(train.churn_proba >= t,1,0)),\n",
    "               \"cantidad\":train.loc[train.churn_proba >= t].shape[0]/train.shape[0]})\n",
    "df_tresh_train = pd.DataFrame(l_)\n",
    "\n",
    "df_tresh_train.to_csv(\"errores_train.csv\", index = False)\n",
    "df_tresh_valid.to_csv(\"errores_validation.csv\", index = False)\n",
    "df_tresh_test.to_csv(\"errores_test.csv\", index = False)\n",
    "\n",
    "\n",
    "bqs.upload_file(\"errores_train.csv\", gcps_path_out + \"errores_train.csv\")\n",
    "bqs.upload_file(\"errores_validation.csv\", gcps_path_out + \"errores_validation.csv\")\n",
    "bqs.upload_file(\"errores_test.csv\", gcps_path_out + \"errores_test.csv\")\n",
    "\n",
    "tresh = df_tresh_test.loc[df_tresh_test.precision >= 0.6].tresh.min()\n",
    "cant = df_tresh_test.loc[df_tresh_test.tresh == tresh].cantidad.values[0]\n",
    "pre = df_tresh_test.loc[df_tresh_test.tresh == tresh].precision.values[0]\n",
    "recall = df_tresh_test.loc[df_tresh_test.tresh == tresh].recall.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thresh_cant_pre = pd.DataFrame(columns = ['Threshold','Cantidad','Precision'])\n",
    "thresh_cant_pre = thresh_cant_pre.append({'Threshold': tresh,'Cantidad':cant,'Precision':pre}, ignore_index = True)\n",
    "thresh_cant_pre.to_csv(\"thresh_cant_pre.csv\", index = False)\n",
    "bqs.upload_file(\"thresh_cant_pre.csv\", gcps_path_out + \"thresh_cant_pre.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision - Recall\n",
    "sns.lineplot(data=df_tresh_train, x=\"recall\", y=\"precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "bines = 50\n",
    "df2 = test.copy()\n",
    "\n",
    "df2['buckets'] = pd.qcut(df2['churn_proba'].rank(method='first'),bines,labels=list(range(bines)))\n",
    "agregados = df2.groupby('buckets').churn.mean()\n",
    "plt.plot(agregados)\n",
    "plt.hlines(y=df2.churn.mean(),xmin=0,xmax=bines-1,color=\"black\")\n",
    "\n",
    "tresh_ind = agregados[agregados>df2.churn.mean()].sort_values().index.min()\n",
    "print(\"Treshold sugerido:\",df2.groupby('buckets').churn_proba.min()[tresh_ind])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict = {\n",
    "            'rand': 123\n",
    "        }\n",
    "    \n",
    "workspace.save_metrics(metrics_dict)\n",
    "\n",
    "import pickle\n",
    "df = pd.DataFrame([1,2,4,5],columns = [\"hola\"])\n",
    "\n",
    "serialized_dataset = pickle.dumps(df)\n",
    "workspace.save_raw_model(serialized_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
